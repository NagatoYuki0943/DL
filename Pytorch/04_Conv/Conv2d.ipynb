{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conv2d\n",
    "\n",
    "math:`(N, C_{\\text{in}}, H, W)` and output :math:`(N, C_{\\text{out}}, H_{\\text{out}}, W_{\\text{out}})`\n",
    "\n",
    "can be precisely described as:\n",
    "\n",
    "$$\n",
    "\\text{out}(N_i, C_{\\text{out}_j}) = \\text{bias}(C_{\\text{out}_j}) +\n",
    "\\sum_{k = 0}^{C_{\\text{in}} - 1} \\text{weight}(C_{\\text{out}_j}, k) \\star \\text{input}(N_i, k)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shape:\n",
    "- Input: :math:`(N, C_{in}, H_{in}, W_{in})` or :math:`(C_{in}, H_{in}, W_{in})`\n",
    "- Output: :math:`(N, C_{out}, H_{out}, W_{out})` or :math:`(C_{out}, H_{out}, W_{out})`, where\n",
    "\n",
    "$$\n",
    "H_{out} = \\left\\lfloor\\frac{H_{in}  + 2 \\times \\text{padding}[0] - \\text{dilation}[0]\n",
    "        \\times (\\text{kernel\\_size}[0] - 1) - 1}{\\text{stride}[0]} + 1\\right\\rfloor\n",
    "$$\n",
    "\n",
    "$$\n",
    "W_{out} = \\left\\lfloor\\frac{W_{in}  + 2 \\times \\text{padding}[1] - \\text{dilation}[1]\n",
    "        \\times (\\text{kernel\\_size}[1] - 1) - 1}{\\text{stride}[1]} + 1\\right\\rfloor\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实例化Conv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(2, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d = nn.Conv2d(\n",
    "    in_channels=2,\n",
    "    out_channels=1,\n",
    "    kernel_size=3,\n",
    "    stride=1,\n",
    "    padding=1,\n",
    "    dilation=1,\n",
    "    groups=1,\n",
    "    bias=True,\n",
    ")\n",
    "conv2d.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 初始化方式1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Conv2d:\n",
    "        nn.init.normal_(m.weight, mean=0, std=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(2, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d.apply(init_weights)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 查看并设置Conv2d的weight和bias(初始化方式2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.0005, -0.0096, -0.0118],\n",
       "          [ 0.0166, -0.0111, -0.0106],\n",
       "          [-0.0021,  0.0072, -0.0109]],\n",
       "\n",
       "         [[-0.0060, -0.0164,  0.0004],\n",
       "          [-0.0056,  0.0084,  0.0112],\n",
       "          [-0.0231,  0.0142, -0.0080]]]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d.weight.data.normal_(0, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.0005, -0.0096, -0.0118],\n",
       "          [ 0.0166, -0.0111, -0.0106],\n",
       "          [-0.0021,  0.0072, -0.0109]],\n",
       "\n",
       "         [[-0.0060, -0.0164,  0.0004],\n",
       "          [-0.0056,  0.0084,  0.0112],\n",
       "          [-0.0231,  0.0142, -0.0080]]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d.bias.data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自动推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.,  2.,  3.,  4.,  5.],\n",
       "          [ 6.,  7.,  8.,  9., 10.],\n",
       "          [11., 12., 13., 14., 15.],\n",
       "          [16., 17., 18., 19., 20.],\n",
       "          [21., 22., 23., 24., 25.]],\n",
       "\n",
       "         [[26., 27., 28., 29., 30.],\n",
       "          [31., 32., 33., 34., 35.],\n",
       "          [36., 37., 38., 39., 40.],\n",
       "          [41., 42., 43., 44., 45.],\n",
       "          [46., 47., 48., 49., 50.]]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(1, 51.0).reshape(1, 2, 5, 5)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 5, 5])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape\n",
    "# (B, C, H, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-16.6096, -17.8440, -18.2492, -18.6544,  -7.1371],\n",
       "          [-19.1482, -23.9443, -24.6553, -25.3664, -10.7965],\n",
       "          [-23.0593, -27.4994, -28.2105, -28.9215, -12.4913],\n",
       "          [-26.9704, -31.0546, -31.7656, -32.4766, -14.1861],\n",
       "          [-19.1191, -17.0690, -17.4659, -17.8627, -11.7871]]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    y = conv2d(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 手动计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-21.7098)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x[:, :, :3, :3] * conv2d.weight.data + conv2d.bias.data).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConvNormAct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNormAct(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: int = 3,\n",
    "        stride: int = 1,\n",
    "        padding: int | None = None,\n",
    "        dilation: int = 1,\n",
    "        groups: int = 1,\n",
    "        bias: bool = True,\n",
    "        norm: nn.Module = nn.BatchNorm2d,\n",
    "        act: nn.Module = nn.ReLU,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        assert in_channels % groups == 0\n",
    "        assert out_channels % groups == 0\n",
    "        padding = padding or dilation * (kernel_size - 1) // 2\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "            dilation=dilation,\n",
    "            groups=groups,\n",
    "            bias=bias,\n",
    "        )\n",
    "        self.norm = norm(out_channels)\n",
    "        self.act = act()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.act(self.norm(self.conv(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
