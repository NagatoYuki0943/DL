{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models\n",
    "from torch.utils.mobile_optimizer import optimize_for_mobile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.shufflenet_v2_x0_5()\n",
    "model.fc.weight.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(1, 3, 224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1000])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x).shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch.jit.trace"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace_model = torch.jit.trace(model.cuda(), example_inputs=x.cuda())\n",
    "trace_model.fc.weight.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.jit.save(trace_model, r\"shufflenet_v2_x0_5.gpu.torchscript\")\n",
    "# trace_model.save(\"shufflenet_v2_x0_5.torchscript\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace_model_ = torch.jit.load(r\"shufflenet_v2_x0_5.gpu.torchscript\")\n",
    "trace_model_.fc.weight.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1000])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace_model_(x.cuda()).shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_model = torch.jit.trace(model.cpu(), example_inputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.jit.save(trace_model, r\"shufflenet_v2_x0_5.cpu.torchscript\")\n",
    "# trace_model.save(\"shufflenet_v2_x0_5.torchscript\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace_model_ = torch.jit.load(r\"shufflenet_v2_x0_5.cpu.torchscript\")\n",
    "trace_model_.fc.weight.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1000])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace_model_(x).shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# optimize_for_mobile 只能使用cpu的模型"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## backend=\"CPU\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize_for_mobile(trace_model, backend=\"CPU\") \\\n",
    "    ._save_for_lite_interpreter(r\"shufflenet_v2_x0_5-optimize-CPU.torchscript\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_model_mobile = torch.jit.load(r\"shufflenet_v2_x0_5-optimize-CPU.torchscript\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1000])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace_model_mobile(x.cpu()).shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## backend=\"Vulkan\" 需要编译支持Vulcan的Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "0 INTERNAL ASSERT FAILED at \"..\\\\torch\\\\csrc\\\\jit\\\\ir\\\\alias_analysis.cpp\":621, please report a bug to PyTorch. We don't have an op for vulkan_prepack::create_linear_context but it isn't a special case.  Argument types: Tensor, Tensor, \n\nCandidates:",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[85], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m optimize_for_mobile(trace_model, backend\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mVulkan\u001b[39;49m\u001b[39m\"\u001b[39;49m) \\\n\u001b[0;32m      2\u001b[0m     \u001b[39m.\u001b[39m_save_for_lite_interpreter(\u001b[39m\"\u001b[39m\u001b[39mshufflenet_v2_x0_5-optimize-Vulkan.torchscript\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\mobile_optimizer.py:67\u001b[0m, in \u001b[0;36moptimize_for_mobile\u001b[1;34m(script_module, optimization_blocklist, preserved_methods, backend)\u001b[0m\n\u001b[0;32m     62\u001b[0m     optimized_cpp_module \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_jit_pass_optimize_for_mobile(\n\u001b[0;32m     63\u001b[0m         script_module\u001b[39m.\u001b[39m_c,\n\u001b[0;32m     64\u001b[0m         optimization_blocklist,\n\u001b[0;32m     65\u001b[0m         preserved_methods_str)\n\u001b[0;32m     66\u001b[0m \u001b[39melif\u001b[39;00m backend \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mvulkan\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m---> 67\u001b[0m     optimized_cpp_module \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_jit_pass_vulkan_optimize_for_mobile(\n\u001b[0;32m     68\u001b[0m         script_module\u001b[39m.\u001b[39;49m_c,\n\u001b[0;32m     69\u001b[0m         optimization_blocklist,\n\u001b[0;32m     70\u001b[0m         preserved_methods_str)\n\u001b[0;32m     71\u001b[0m \u001b[39melif\u001b[39;00m backend \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmetal\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m     72\u001b[0m     optimized_cpp_module \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_jit_pass_metal_optimize_for_mobile(script_module\u001b[39m.\u001b[39m_c, preserved_methods_str)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: 0 INTERNAL ASSERT FAILED at \"..\\\\torch\\\\csrc\\\\jit\\\\ir\\\\alias_analysis.cpp\":621, please report a bug to PyTorch. We don't have an op for vulkan_prepack::create_linear_context but it isn't a special case.  Argument types: Tensor, Tensor, \n\nCandidates:"
     ]
    }
   ],
   "source": [
    "optimize_for_mobile(trace_model, backend=\"Vulkan\") \\\n",
    "    ._save_for_lite_interpreter(\"shufflenet_v2_x0_5-optimize-Vulkan.torchscript\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## backend=\"Metal\" 需要编译支持Metal的Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "0 INTERNAL ASSERT FAILED at \"..\\\\torch\\\\csrc\\\\jit\\\\ir\\\\alias_analysis.cpp\":621, please report a bug to PyTorch. We don't have an op for metal_prepack::linear_prepack but it isn't a special case.  Argument types: Tensor, Tensor, NoneType, NoneType, \n\nCandidates:",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[86], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m optimize_for_mobile(trace_model, backend\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mMetal\u001b[39;49m\u001b[39m\"\u001b[39;49m) \\\n\u001b[0;32m      2\u001b[0m     \u001b[39m.\u001b[39m_save_for_lite_interpreter(\u001b[39m\"\u001b[39m\u001b[39mshufflenet_v2_x0_5-optimize-Metal.torchscript\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\mobile_optimizer.py:72\u001b[0m, in \u001b[0;36moptimize_for_mobile\u001b[1;34m(script_module, optimization_blocklist, preserved_methods, backend)\u001b[0m\n\u001b[0;32m     67\u001b[0m     optimized_cpp_module \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_jit_pass_vulkan_optimize_for_mobile(\n\u001b[0;32m     68\u001b[0m         script_module\u001b[39m.\u001b[39m_c,\n\u001b[0;32m     69\u001b[0m         optimization_blocklist,\n\u001b[0;32m     70\u001b[0m         preserved_methods_str)\n\u001b[0;32m     71\u001b[0m \u001b[39melif\u001b[39;00m backend \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmetal\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m---> 72\u001b[0m     optimized_cpp_module \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_jit_pass_metal_optimize_for_mobile(script_module\u001b[39m.\u001b[39;49m_c, preserved_methods_str)\n\u001b[0;32m     73\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     74\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mUnknown backend, must be one of \u001b[39m\u001b[39m'\u001b[39m\u001b[39mCPU\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39mVulkan\u001b[39m\u001b[39m'\u001b[39m\u001b[39m or \u001b[39m\u001b[39m'\u001b[39m\u001b[39mMetal\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: 0 INTERNAL ASSERT FAILED at \"..\\\\torch\\\\csrc\\\\jit\\\\ir\\\\alias_analysis.cpp\":621, please report a bug to PyTorch. We don't have an op for metal_prepack::linear_prepack but it isn't a special case.  Argument types: Tensor, Tensor, NoneType, NoneType, \n\nCandidates:"
     ]
    }
   ],
   "source": [
    "optimize_for_mobile(trace_model, backend=\"Metal\") \\\n",
    "    ._save_for_lite_interpreter(\"shufflenet_v2_x0_5-optimize-Metal.torchscript\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
