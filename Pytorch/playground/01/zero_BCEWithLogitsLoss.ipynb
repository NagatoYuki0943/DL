{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01分类\n",
    "\n",
    "小于等于0为0,否则为1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=1, out_features=3, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=3, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(1, 3),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(3, 1),\n",
    ").to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGD (\n",
       "Parameter Group 0\n",
       "    dampening: 0\n",
       "    differentiable: False\n",
       "    foreach: None\n",
       "    lr: 0.1\n",
       "    maximize: False\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BCEWithLogitsLoss()"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 要求模型输出个数为1\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "loss_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2869],\n",
       "        [-2.7048],\n",
       "        [ 0.8248],\n",
       "        ...,\n",
       "        [ 0.4532],\n",
       "        [ 0.2260],\n",
       "        [-0.3508]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x = torch.randn(10000, 1)\n",
    "train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        ...,\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y = (train_x > 0).type(torch.float)\n",
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4051],\n",
       "        [ 0.0679],\n",
       "        [-0.3509],\n",
       "        ...,\n",
       "        [ 0.0711],\n",
       "        [-0.3738],\n",
       "        [ 0.3602]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_x = torch.rand([10000, 1]) * 2 - 1\n",
    "val_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        ...,\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_y = (val_x > 0).type(torch.float)\n",
    "val_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x216b986fa90>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader = DataLoader(Data(train_x, train_y), batch_size=1000, shuffle=True)\n",
    "train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x216b986d910>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataloader = DataLoader(Data(val_x, val_y), batch_size=1000, shuffle=True)\n",
    "val_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, train, acc: 0.128900, loss: 0.761887\n",
      "epoch: 1, val, acc: 0.506600, loss: 0.705111\n",
      "epoch: 2, train, acc: 0.487000, loss: 0.687132\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2, val, acc: 0.506600, loss: 0.670082\n",
      "epoch: 3, train, acc: 0.633300, loss: 0.637318\n",
      "epoch: 3, val, acc: 0.703100, loss: 0.633289\n",
      "epoch: 4, train, acc: 0.829200, loss: 0.582872\n",
      "epoch: 4, val, acc: 0.838200, loss: 0.587674\n",
      "epoch: 5, train, acc: 0.900900, loss: 0.520129\n",
      "epoch: 5, val, acc: 0.911200, loss: 0.533740\n",
      "epoch: 6, train, acc: 0.943200, loss: 0.453638\n",
      "epoch: 6, val, acc: 0.951200, loss: 0.475961\n",
      "epoch: 7, train, acc: 0.970300, loss: 0.390047\n",
      "epoch: 7, val, acc: 0.971800, loss: 0.420134\n",
      "epoch: 8, train, acc: 0.982000, loss: 0.334677\n",
      "epoch: 8, val, acc: 0.983600, loss: 0.370869\n",
      "epoch: 9, train, acc: 0.988800, loss: 0.289440\n",
      "epoch: 9, val, acc: 0.990600, loss: 0.329393\n",
      "epoch: 10, train, acc: 0.994100, loss: 0.253618\n",
      "epoch: 10, val, acc: 0.993600, loss: 0.295208\n",
      "epoch: 11, train, acc: 0.995100, loss: 0.225451\n",
      "epoch: 11, val, acc: 0.995500, loss: 0.267243\n",
      "epoch: 12, train, acc: 0.997300, loss: 0.203156\n",
      "epoch: 12, val, acc: 0.996400, loss: 0.244267\n",
      "epoch: 13, train, acc: 0.997700, loss: 0.185292\n",
      "epoch: 13, val, acc: 0.997300, loss: 0.225224\n",
      "epoch: 14, train, acc: 0.998000, loss: 0.170754\n",
      "epoch: 14, val, acc: 0.997700, loss: 0.209281\n",
      "epoch: 15, train, acc: 0.998200, loss: 0.158748\n",
      "epoch: 15, val, acc: 0.997900, loss: 0.195779\n",
      "epoch: 16, train, acc: 0.997800, loss: 0.148668\n",
      "epoch: 16, val, acc: 0.998300, loss: 0.184219\n",
      "epoch: 17, train, acc: 0.998500, loss: 0.140097\n",
      "epoch: 17, val, acc: 0.998400, loss: 0.174230\n",
      "epoch: 18, train, acc: 0.998800, loss: 0.132732\n",
      "epoch: 18, val, acc: 0.998200, loss: 0.165516\n",
      "epoch: 19, train, acc: 0.998300, loss: 0.126314\n",
      "epoch: 19, val, acc: 0.998400, loss: 0.157839\n",
      "epoch: 20, train, acc: 0.998600, loss: 0.120684\n",
      "epoch: 20, val, acc: 0.998500, loss: 0.151033\n",
      "epoch: 21, train, acc: 0.998600, loss: 0.115694\n",
      "epoch: 21, val, acc: 0.998500, loss: 0.144949\n",
      "epoch: 22, train, acc: 0.998600, loss: 0.111241\n",
      "epoch: 22, val, acc: 0.998600, loss: 0.139481\n",
      "epoch: 23, train, acc: 0.999100, loss: 0.107230\n",
      "epoch: 23, val, acc: 0.998500, loss: 0.134540\n",
      "epoch: 24, train, acc: 0.998900, loss: 0.103604\n",
      "epoch: 24, val, acc: 0.998500, loss: 0.130042\n",
      "epoch: 25, train, acc: 0.998600, loss: 0.100310\n",
      "epoch: 25, val, acc: 0.998800, loss: 0.125931\n",
      "epoch: 26, train, acc: 0.998800, loss: 0.097296\n",
      "epoch: 26, val, acc: 0.998700, loss: 0.122162\n",
      "epoch: 27, train, acc: 0.999100, loss: 0.094532\n",
      "epoch: 27, val, acc: 0.998800, loss: 0.118689\n",
      "epoch: 28, train, acc: 0.998700, loss: 0.091985\n",
      "epoch: 28, val, acc: 0.998800, loss: 0.115479\n",
      "epoch: 29, train, acc: 0.998800, loss: 0.089614\n",
      "epoch: 29, val, acc: 0.998900, loss: 0.112499\n",
      "epoch: 30, train, acc: 0.999100, loss: 0.087436\n",
      "epoch: 30, val, acc: 0.998900, loss: 0.109729\n",
      "epoch: 31, train, acc: 0.999300, loss: 0.085375\n",
      "epoch: 31, val, acc: 0.998800, loss: 0.107144\n",
      "epoch: 32, train, acc: 0.999500, loss: 0.083466\n",
      "epoch: 32, val, acc: 0.998600, loss: 0.104725\n",
      "epoch: 33, train, acc: 0.999000, loss: 0.081666\n",
      "epoch: 33, val, acc: 0.998900, loss: 0.102449\n",
      "epoch: 34, train, acc: 0.998900, loss: 0.079987\n",
      "epoch: 34, val, acc: 0.999000, loss: 0.100310\n",
      "epoch: 35, train, acc: 0.999200, loss: 0.078405\n",
      "epoch: 35, val, acc: 0.999000, loss: 0.098295\n",
      "epoch: 36, train, acc: 0.999100, loss: 0.076904\n",
      "epoch: 36, val, acc: 0.999300, loss: 0.096390\n",
      "epoch: 37, train, acc: 0.999400, loss: 0.075489\n",
      "epoch: 37, val, acc: 0.999300, loss: 0.094590\n",
      "epoch: 38, train, acc: 0.999200, loss: 0.074141\n",
      "epoch: 38, val, acc: 0.999300, loss: 0.092882\n",
      "epoch: 39, train, acc: 0.998800, loss: 0.072874\n",
      "epoch: 39, val, acc: 0.999400, loss: 0.091258\n",
      "epoch: 40, train, acc: 0.999100, loss: 0.071656\n",
      "epoch: 40, val, acc: 0.999400, loss: 0.089717\n",
      "epoch: 41, train, acc: 0.999500, loss: 0.070502\n",
      "epoch: 41, val, acc: 0.999300, loss: 0.088252\n",
      "epoch: 42, train, acc: 0.999600, loss: 0.069404\n",
      "epoch: 42, val, acc: 0.998900, loss: 0.086856\n",
      "epoch: 43, train, acc: 0.999400, loss: 0.068351\n",
      "epoch: 43, val, acc: 0.998900, loss: 0.085519\n",
      "epoch: 44, train, acc: 0.999400, loss: 0.067343\n",
      "epoch: 44, val, acc: 0.998900, loss: 0.084240\n",
      "epoch: 45, train, acc: 0.999400, loss: 0.066380\n",
      "epoch: 45, val, acc: 0.998900, loss: 0.083015\n",
      "epoch: 46, train, acc: 0.999000, loss: 0.065462\n",
      "epoch: 46, val, acc: 0.999300, loss: 0.081835\n",
      "epoch: 47, train, acc: 0.999400, loss: 0.064581\n",
      "epoch: 47, val, acc: 0.999300, loss: 0.080707\n",
      "epoch: 48, train, acc: 0.999400, loss: 0.063722\n",
      "epoch: 48, val, acc: 0.999200, loss: 0.079626\n",
      "epoch: 49, train, acc: 0.999400, loss: 0.062904\n",
      "epoch: 49, val, acc: 0.999000, loss: 0.078586\n",
      "epoch: 50, train, acc: 0.999300, loss: 0.062112\n",
      "epoch: 50, val, acc: 0.999200, loss: 0.077582\n",
      "epoch: 51, train, acc: 0.999400, loss: 0.061350\n",
      "epoch: 51, val, acc: 0.999100, loss: 0.076617\n",
      "epoch: 52, train, acc: 0.999300, loss: 0.060625\n",
      "epoch: 52, val, acc: 0.999300, loss: 0.075684\n",
      "epoch: 53, train, acc: 0.999400, loss: 0.059917\n",
      "epoch: 53, val, acc: 0.999300, loss: 0.074785\n",
      "epoch: 54, train, acc: 0.999300, loss: 0.059237\n",
      "epoch: 54, val, acc: 0.999300, loss: 0.073916\n",
      "epoch: 55, train, acc: 0.999500, loss: 0.058573\n",
      "epoch: 55, val, acc: 0.999100, loss: 0.073080\n",
      "epoch: 56, train, acc: 0.999400, loss: 0.057926\n",
      "epoch: 56, val, acc: 0.999300, loss: 0.072266\n",
      "epoch: 57, train, acc: 0.999300, loss: 0.057307\n",
      "epoch: 57, val, acc: 0.999300, loss: 0.071480\n",
      "epoch: 58, train, acc: 0.999500, loss: 0.056706\n",
      "epoch: 58, val, acc: 0.999300, loss: 0.070721\n",
      "epoch: 59, train, acc: 0.999300, loss: 0.056127\n",
      "epoch: 59, val, acc: 0.999300, loss: 0.069982\n",
      "epoch: 60, train, acc: 0.999500, loss: 0.055570\n",
      "epoch: 60, val, acc: 0.999300, loss: 0.069267\n",
      "epoch: 61, train, acc: 0.999400, loss: 0.055016\n",
      "epoch: 61, val, acc: 0.999300, loss: 0.068574\n",
      "epoch: 62, train, acc: 0.999400, loss: 0.054487\n",
      "epoch: 62, val, acc: 0.999400, loss: 0.067900\n",
      "epoch: 63, train, acc: 0.999400, loss: 0.053964\n",
      "epoch: 63, val, acc: 0.999400, loss: 0.067244\n",
      "epoch: 64, train, acc: 0.999400, loss: 0.053471\n",
      "epoch: 64, val, acc: 0.999400, loss: 0.066608\n",
      "epoch: 65, train, acc: 0.999400, loss: 0.052977\n",
      "epoch: 65, val, acc: 0.999400, loss: 0.065989\n",
      "epoch: 66, train, acc: 0.999500, loss: 0.052507\n",
      "epoch: 66, val, acc: 0.999400, loss: 0.065389\n",
      "epoch: 67, train, acc: 0.999400, loss: 0.052034\n",
      "epoch: 67, val, acc: 0.999400, loss: 0.064802\n",
      "epoch: 68, train, acc: 0.999500, loss: 0.051589\n",
      "epoch: 68, val, acc: 0.999400, loss: 0.064232\n",
      "epoch: 69, train, acc: 0.999400, loss: 0.051148\n",
      "epoch: 69, val, acc: 0.999400, loss: 0.063676\n",
      "epoch: 70, train, acc: 0.999700, loss: 0.050714\n",
      "epoch: 70, val, acc: 0.999400, loss: 0.063136\n",
      "epoch: 71, train, acc: 0.999500, loss: 0.050299\n",
      "epoch: 71, val, acc: 0.999400, loss: 0.062607\n",
      "epoch: 72, train, acc: 0.999500, loss: 0.049889\n",
      "epoch: 72, val, acc: 0.999400, loss: 0.062091\n",
      "epoch: 73, train, acc: 0.999400, loss: 0.049494\n",
      "epoch: 73, val, acc: 0.999400, loss: 0.061588\n",
      "epoch: 74, train, acc: 0.999500, loss: 0.049100\n",
      "epoch: 74, val, acc: 0.999400, loss: 0.061099\n",
      "epoch: 75, train, acc: 0.999400, loss: 0.048721\n",
      "epoch: 75, val, acc: 0.999400, loss: 0.060618\n",
      "epoch: 76, train, acc: 0.999700, loss: 0.048357\n",
      "epoch: 76, val, acc: 0.999400, loss: 0.060152\n",
      "epoch: 77, train, acc: 0.999500, loss: 0.047991\n",
      "epoch: 77, val, acc: 0.999300, loss: 0.059695\n",
      "epoch: 78, train, acc: 0.999500, loss: 0.047630\n",
      "epoch: 78, val, acc: 0.999300, loss: 0.059248\n",
      "epoch: 79, train, acc: 0.999500, loss: 0.047282\n",
      "epoch: 79, val, acc: 0.999400, loss: 0.058809\n",
      "epoch: 80, train, acc: 0.999500, loss: 0.046941\n",
      "epoch: 80, val, acc: 0.999400, loss: 0.058381\n",
      "epoch: 81, train, acc: 0.999800, loss: 0.046614\n",
      "epoch: 81, val, acc: 0.999300, loss: 0.057964\n",
      "epoch: 82, train, acc: 0.999300, loss: 0.046282\n",
      "epoch: 82, val, acc: 0.999400, loss: 0.057551\n",
      "epoch: 83, train, acc: 0.999500, loss: 0.045964\n",
      "epoch: 83, val, acc: 0.999400, loss: 0.057149\n",
      "epoch: 84, train, acc: 0.999500, loss: 0.045656\n",
      "epoch: 84, val, acc: 0.999400, loss: 0.056755\n",
      "epoch: 85, train, acc: 0.999700, loss: 0.045353\n",
      "epoch: 85, val, acc: 0.999400, loss: 0.056371\n",
      "epoch: 86, train, acc: 0.999600, loss: 0.045044\n",
      "epoch: 86, val, acc: 0.999300, loss: 0.055996\n",
      "epoch: 87, train, acc: 0.999400, loss: 0.044746\n",
      "epoch: 87, val, acc: 0.999400, loss: 0.055624\n",
      "epoch: 88, train, acc: 0.999400, loss: 0.044459\n",
      "epoch: 88, val, acc: 0.999400, loss: 0.055261\n",
      "epoch: 89, train, acc: 0.999800, loss: 0.044181\n",
      "epoch: 89, val, acc: 0.999400, loss: 0.054906\n",
      "epoch: 90, train, acc: 0.999400, loss: 0.043904\n",
      "epoch: 90, val, acc: 0.999400, loss: 0.054556\n",
      "epoch: 91, train, acc: 0.999500, loss: 0.043619\n",
      "epoch: 91, val, acc: 0.999400, loss: 0.054213\n",
      "epoch: 92, train, acc: 0.999500, loss: 0.043361\n",
      "epoch: 92, val, acc: 0.999400, loss: 0.053876\n",
      "epoch: 93, train, acc: 0.999800, loss: 0.043093\n",
      "epoch: 93, val, acc: 0.999400, loss: 0.053546\n",
      "epoch: 94, train, acc: 0.999500, loss: 0.042841\n",
      "epoch: 94, val, acc: 0.999400, loss: 0.053222\n",
      "epoch: 95, train, acc: 0.999600, loss: 0.042584\n",
      "epoch: 95, val, acc: 0.999400, loss: 0.052904\n",
      "epoch: 96, train, acc: 0.999600, loss: 0.042330\n",
      "epoch: 96, val, acc: 0.999400, loss: 0.052592\n",
      "epoch: 97, train, acc: 0.999600, loss: 0.042086\n",
      "epoch: 97, val, acc: 0.999400, loss: 0.052284\n",
      "epoch: 98, train, acc: 0.999800, loss: 0.041843\n",
      "epoch: 98, val, acc: 0.999400, loss: 0.051983\n",
      "epoch: 99, train, acc: 0.999500, loss: 0.041610\n",
      "epoch: 99, val, acc: 0.999500, loss: 0.051685\n",
      "epoch: 100, train, acc: 0.999800, loss: 0.041375\n",
      "epoch: 100, val, acc: 0.999400, loss: 0.051394\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs+1):\n",
    "    model.train()\n",
    "    y_trues = []\n",
    "    y_preds = []\n",
    "    losses = []\n",
    "    for x, y in train_dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred: Tensor = model(x)\n",
    "        loss: Tensor = loss_fn(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        y_trues.append(y)\n",
    "        y_preds.append((y_pred.sigmoid() > 0.5).type(torch.float))    # 大于0.5代表正确\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    acc = (torch.cat(y_trues) == torch.cat(y_preds)).type(torch.float).mean().item()\n",
    "    loss_mean = torch.tensor(losses).mean().item()\n",
    "    print(f\"epoch: {epoch}, train, acc: {acc:.6f}, loss: {loss_mean:.6f}\")\n",
    "\n",
    "    model.eval()\n",
    "    y_trues = []\n",
    "    y_preds = []\n",
    "    losses = []\n",
    "    for x, y in val_dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        with torch.no_grad():\n",
    "            y_pred: Tensor = model(x)\n",
    "        loss: Tensor = loss_fn(y_pred, y)\n",
    "        y_trues.append(y)\n",
    "        y_preds.append((y_pred.sigmoid() > 0.5).type(torch.float))    # 大于0.5代表正确\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    acc = (torch.cat(y_trues) == torch.cat(y_preds)).type(torch.float).mean().item()\n",
    "    loss_mean = torch.tensor(losses).mean().item()\n",
    "    print(f\"epoch: {epoch}, val, acc: {acc:.6f}, loss: {loss_mean:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4564],\n",
      "        [0.4909],\n",
      "        [0.4987],\n",
      "        [0.5333]])\n"
     ]
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    x = torch.tensor([-0.01, -0.001, 0.001, 0.01]).reshape(-1, 1).to(device)\n",
    "    print(model(x).cpu().sigmoid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000e-04, 1.0975e-04, 1.2045e-04, 1.3219e-04, 1.4508e-04, 1.5923e-04,\n",
       "        1.7475e-04, 1.9179e-04, 2.1049e-04, 2.3101e-04, 2.5354e-04, 2.7826e-04,\n",
       "        3.0539e-04, 3.3516e-04, 3.6784e-04, 4.0370e-04, 4.4306e-04, 4.8626e-04,\n",
       "        5.3367e-04, 5.8570e-04, 6.4281e-04, 7.0548e-04, 7.7426e-04, 8.4975e-04,\n",
       "        9.3260e-04, 1.0235e-03, 1.1233e-03, 1.2328e-03, 1.3530e-03, 1.4850e-03,\n",
       "        1.6298e-03, 1.7886e-03, 1.9630e-03, 2.1544e-03, 2.3645e-03, 2.5950e-03,\n",
       "        2.8480e-03, 3.1257e-03, 3.4305e-03, 3.7649e-03, 4.1320e-03, 4.5349e-03,\n",
       "        4.9770e-03, 5.4623e-03, 5.9948e-03, 6.5793e-03, 7.2208e-03, 7.9248e-03,\n",
       "        8.6975e-03, 9.5455e-03, 1.0476e-02, 1.1498e-02, 1.2619e-02, 1.3849e-02,\n",
       "        1.5199e-02, 1.6681e-02, 1.8307e-02, 2.0092e-02, 2.2051e-02, 2.4201e-02,\n",
       "        2.6561e-02, 2.9151e-02, 3.1993e-02, 3.5112e-02, 3.8535e-02, 4.2292e-02,\n",
       "        4.6416e-02, 5.0941e-02, 5.5908e-02, 6.1359e-02, 6.7342e-02, 7.3907e-02,\n",
       "        8.1113e-02, 8.9022e-02, 9.7701e-02, 1.0723e-01, 1.1768e-01, 1.2915e-01,\n",
       "        1.4175e-01, 1.5557e-01, 1.7074e-01, 1.8738e-01, 2.0565e-01, 2.2570e-01,\n",
       "        2.4771e-01, 2.7186e-01, 2.9836e-01, 3.2745e-01, 3.5938e-01, 3.9442e-01,\n",
       "        4.3288e-01, 4.7508e-01, 5.2140e-01, 5.7224e-01, 6.2803e-01, 6.8926e-01,\n",
       "        7.5646e-01, 8.3022e-01, 9.1116e-01, 1.0000e+00])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x = torch.logspace(-4, 0, 100)\n",
    "test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200, 1])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x = torch.cat([test_x, -test_x]).reshape(-1, 1)\n",
    "test_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y = (test_x > 0).type(torch.float)\n",
    "test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.inference_mode():\n",
    "    y_pred = (model(test_x.to(device)).cpu().sigmoid() > 0.5).type(torch.float)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8600000143051147"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 准确率\n",
    "(test_y == y_pred).type(torch.float).mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获取错误的 x index\n",
    "error_index = (test_y != y_pred)\n",
    "error_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200, 1])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000e-04],\n",
       "        [1.0975e-04],\n",
       "        [1.2045e-04],\n",
       "        [1.3219e-04],\n",
       "        [1.4508e-04],\n",
       "        [1.5923e-04],\n",
       "        [1.7475e-04],\n",
       "        [1.9179e-04],\n",
       "        [2.1049e-04],\n",
       "        [2.3101e-04],\n",
       "        [2.5354e-04],\n",
       "        [2.7826e-04],\n",
       "        [3.0539e-04],\n",
       "        [3.3516e-04],\n",
       "        [3.6784e-04],\n",
       "        [4.0370e-04],\n",
       "        [4.4306e-04],\n",
       "        [4.8626e-04],\n",
       "        [5.3367e-04],\n",
       "        [5.8570e-04],\n",
       "        [6.4281e-04],\n",
       "        [7.0548e-04],\n",
       "        [7.7426e-04],\n",
       "        [8.4975e-04],\n",
       "        [9.3260e-04],\n",
       "        [1.0235e-03],\n",
       "        [1.1233e-03],\n",
       "        [1.2328e-03]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获取错误的 x\n",
    "error_x = test_x[error_index.flatten()]\n",
    "error_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_x.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
