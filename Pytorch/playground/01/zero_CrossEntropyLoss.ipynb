{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int):\n",
    "    torch.seed(seed)\n",
    "    torch.cuda.seed_all(seed)\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01分类\n",
    "\n",
    "小于等于0为0,否则为1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=1, out_features=3, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=3, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(1, 3),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(3, 2),\n",
    ").to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGD (\n",
       "Parameter Group 0\n",
       "    dampening: 0\n",
       "    differentiable: False\n",
       "    foreach: None\n",
       "    lr: 0.1\n",
       "    maximize: False\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrossEntropyLoss()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "loss_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2403],\n",
       "        [-0.0928],\n",
       "        [ 0.0116],\n",
       "        ...,\n",
       "        [ 0.3054],\n",
       "        [ 2.5811],\n",
       "        [-2.4365]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x = torch.randn(10000, 1)\n",
    "train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 1,  ..., 1, 1, 0])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y = (train_x > 0).type(torch.long).flatten()\n",
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2275],\n",
       "        [-0.2959],\n",
       "        [-0.9763],\n",
       "        ...,\n",
       "        [-0.4862],\n",
       "        [-0.1284],\n",
       "        [-0.1013]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_x = torch.rand([10000, 1]) * 2 - 1\n",
    "val_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 0,  ..., 0, 0, 0])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_y = (val_x > 0).type(torch.long).flatten()\n",
    "val_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x28ebb237490>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader = DataLoader(Data(train_x, train_y), batch_size=1000, shuffle=True)\n",
    "train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x28ebe443890>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataloader = DataLoader(Data(val_x, val_y), batch_size=1000, shuffle=False)\n",
    "val_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5583, 0.4417],\n",
      "        [0.5161, 0.4839],\n",
      "        [0.5067, 0.4933],\n",
      "        [0.4642, 0.5358]])\n"
     ]
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    x = torch.tensor([-0.01, -0.001, 0.001, 0.01]).reshape(-1, 1).to(device)\n",
    "    print(model(x).cpu().softmax(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, train, acc: 0.502000, loss: 0.689267\n",
      "epoch: 1, val, acc: 0.500100, loss: 0.653449\n",
      "epoch: 2, train, acc: 0.679600, loss: 0.604887\n",
      "epoch: 2, val, acc: 0.834900, loss: 0.595620\n",
      "epoch: 3, train, acc: 0.909800, loss: 0.522756\n",
      "epoch: 3, val, acc: 0.929900, loss: 0.527265\n",
      "epoch: 4, train, acc: 0.956900, loss: 0.435517\n",
      "epoch: 4, val, acc: 0.959900, loss: 0.452944\n",
      "epoch: 5, train, acc: 0.975000, loss: 0.356489\n",
      "epoch: 5, val, acc: 0.973700, loss: 0.384562\n",
      "epoch: 6, train, acc: 0.981500, loss: 0.293868\n",
      "epoch: 6, val, acc: 0.981800, loss: 0.328405\n",
      "epoch: 7, train, acc: 0.985800, loss: 0.247217\n",
      "epoch: 7, val, acc: 0.985900, loss: 0.284430\n",
      "epoch: 8, train, acc: 0.989200, loss: 0.212807\n",
      "epoch: 8, val, acc: 0.987300, loss: 0.250203\n",
      "epoch: 9, train, acc: 0.990300, loss: 0.186901\n",
      "epoch: 9, val, acc: 0.988200, loss: 0.223130\n",
      "epoch: 10, train, acc: 0.991600, loss: 0.166760\n",
      "epoch: 10, val, acc: 0.989400, loss: 0.201136\n",
      "epoch: 11, train, acc: 0.992500, loss: 0.150551\n",
      "epoch: 11, val, acc: 0.991800, loss: 0.183068\n",
      "epoch: 12, train, acc: 0.993900, loss: 0.137586\n",
      "epoch: 12, val, acc: 0.994600, loss: 0.168557\n",
      "epoch: 13, train, acc: 0.996900, loss: 0.127186\n",
      "epoch: 13, val, acc: 0.996200, loss: 0.156682\n",
      "epoch: 14, train, acc: 0.998200, loss: 0.118698\n",
      "epoch: 14, val, acc: 0.997200, loss: 0.146828\n",
      "epoch: 15, train, acc: 0.997900, loss: 0.111653\n",
      "epoch: 15, val, acc: 0.998200, loss: 0.138542\n",
      "epoch: 16, train, acc: 0.998500, loss: 0.105674\n",
      "epoch: 16, val, acc: 0.999400, loss: 0.131463\n",
      "epoch: 17, train, acc: 0.999800, loss: 0.100557\n",
      "epoch: 17, val, acc: 0.999600, loss: 0.125336\n",
      "epoch: 18, train, acc: 0.999700, loss: 0.096135\n",
      "epoch: 18, val, acc: 0.999700, loss: 0.119978\n",
      "epoch: 19, train, acc: 0.999600, loss: 0.092243\n",
      "epoch: 19, val, acc: 0.999900, loss: 0.115242\n",
      "epoch: 20, train, acc: 0.999700, loss: 0.088793\n",
      "epoch: 20, val, acc: 1.000000, loss: 0.111026\n",
      "epoch: 21, train, acc: 0.999500, loss: 0.085710\n",
      "epoch: 21, val, acc: 0.999800, loss: 0.107239\n",
      "epoch: 22, train, acc: 0.998900, loss: 0.082926\n",
      "epoch: 22, val, acc: 0.999900, loss: 0.103809\n",
      "epoch: 23, train, acc: 0.999600, loss: 0.080412\n",
      "epoch: 23, val, acc: 0.999600, loss: 0.100700\n",
      "epoch: 24, train, acc: 0.999200, loss: 0.078113\n",
      "epoch: 24, val, acc: 0.999500, loss: 0.097851\n",
      "epoch: 25, train, acc: 0.999100, loss: 0.076005\n",
      "epoch: 25, val, acc: 0.999100, loss: 0.095239\n",
      "epoch: 26, train, acc: 0.998800, loss: 0.074084\n",
      "epoch: 26, val, acc: 0.999200, loss: 0.092819\n",
      "epoch: 27, train, acc: 0.998900, loss: 0.072269\n",
      "epoch: 27, val, acc: 0.999200, loss: 0.090581\n",
      "epoch: 28, train, acc: 0.998600, loss: 0.070612\n",
      "epoch: 28, val, acc: 0.999200, loss: 0.088496\n",
      "epoch: 29, train, acc: 0.999400, loss: 0.069063\n",
      "epoch: 29, val, acc: 0.998900, loss: 0.086563\n",
      "epoch: 30, train, acc: 0.998300, loss: 0.067611\n",
      "epoch: 30, val, acc: 0.999200, loss: 0.084738\n",
      "epoch: 31, train, acc: 0.998800, loss: 0.066254\n",
      "epoch: 31, val, acc: 0.999100, loss: 0.083033\n",
      "epoch: 32, train, acc: 0.998800, loss: 0.064978\n",
      "epoch: 32, val, acc: 0.999200, loss: 0.081426\n",
      "epoch: 33, train, acc: 0.998900, loss: 0.063770\n",
      "epoch: 33, val, acc: 0.999200, loss: 0.079913\n",
      "epoch: 34, train, acc: 0.998800, loss: 0.062632\n",
      "epoch: 34, val, acc: 0.999100, loss: 0.078486\n",
      "epoch: 35, train, acc: 0.998800, loss: 0.061546\n",
      "epoch: 35, val, acc: 0.999300, loss: 0.077128\n",
      "epoch: 36, train, acc: 0.998700, loss: 0.060526\n",
      "epoch: 36, val, acc: 0.999600, loss: 0.075842\n",
      "epoch: 37, train, acc: 0.999000, loss: 0.059562\n",
      "epoch: 37, val, acc: 0.999300, loss: 0.074622\n",
      "epoch: 38, train, acc: 0.999000, loss: 0.058650\n",
      "epoch: 38, val, acc: 0.999000, loss: 0.073463\n",
      "epoch: 39, train, acc: 0.999300, loss: 0.057761\n",
      "epoch: 39, val, acc: 0.998900, loss: 0.072361\n",
      "epoch: 40, train, acc: 0.998900, loss: 0.056910\n",
      "epoch: 40, val, acc: 0.998800, loss: 0.071306\n",
      "epoch: 41, train, acc: 0.998600, loss: 0.056111\n",
      "epoch: 41, val, acc: 0.998900, loss: 0.070288\n",
      "epoch: 42, train, acc: 0.998800, loss: 0.055343\n",
      "epoch: 42, val, acc: 0.998900, loss: 0.069321\n",
      "epoch: 43, train, acc: 0.998800, loss: 0.054599\n",
      "epoch: 43, val, acc: 0.998900, loss: 0.068392\n",
      "epoch: 44, train, acc: 0.998800, loss: 0.053891\n",
      "epoch: 44, val, acc: 0.998800, loss: 0.067501\n",
      "epoch: 45, train, acc: 0.999100, loss: 0.053222\n",
      "epoch: 45, val, acc: 0.998800, loss: 0.066650\n",
      "epoch: 46, train, acc: 0.998600, loss: 0.052565\n",
      "epoch: 46, val, acc: 0.998800, loss: 0.065819\n",
      "epoch: 47, train, acc: 0.998800, loss: 0.051940\n",
      "epoch: 47, val, acc: 0.998900, loss: 0.065021\n",
      "epoch: 48, train, acc: 0.998700, loss: 0.051336\n",
      "epoch: 48, val, acc: 0.999000, loss: 0.064255\n",
      "epoch: 49, train, acc: 0.998600, loss: 0.050748\n",
      "epoch: 49, val, acc: 0.999100, loss: 0.063515\n",
      "epoch: 50, train, acc: 0.999200, loss: 0.050195\n",
      "epoch: 50, val, acc: 0.998900, loss: 0.062807\n",
      "epoch: 51, train, acc: 0.998900, loss: 0.049634\n",
      "epoch: 51, val, acc: 0.999000, loss: 0.062117\n",
      "epoch: 52, train, acc: 0.998900, loss: 0.049111\n",
      "epoch: 52, val, acc: 0.998900, loss: 0.061456\n",
      "epoch: 53, train, acc: 0.998500, loss: 0.048589\n",
      "epoch: 53, val, acc: 0.998900, loss: 0.060809\n",
      "epoch: 54, train, acc: 0.998800, loss: 0.048112\n",
      "epoch: 54, val, acc: 0.998900, loss: 0.060187\n",
      "epoch: 55, train, acc: 0.998600, loss: 0.047628\n",
      "epoch: 55, val, acc: 0.998900, loss: 0.059583\n",
      "epoch: 56, train, acc: 0.998700, loss: 0.047161\n",
      "epoch: 56, val, acc: 0.998900, loss: 0.058996\n",
      "epoch: 57, train, acc: 0.998800, loss: 0.046710\n",
      "epoch: 57, val, acc: 0.998900, loss: 0.058429\n",
      "epoch: 58, train, acc: 0.998500, loss: 0.046269\n",
      "epoch: 58, val, acc: 0.998900, loss: 0.057876\n",
      "epoch: 59, train, acc: 0.998800, loss: 0.045841\n",
      "epoch: 59, val, acc: 0.998800, loss: 0.057343\n",
      "epoch: 60, train, acc: 0.998400, loss: 0.045431\n",
      "epoch: 60, val, acc: 0.998900, loss: 0.056817\n",
      "epoch: 61, train, acc: 0.998800, loss: 0.045034\n",
      "epoch: 61, val, acc: 0.998900, loss: 0.056311\n",
      "epoch: 62, train, acc: 0.998800, loss: 0.044640\n",
      "epoch: 62, val, acc: 0.998900, loss: 0.055819\n",
      "epoch: 63, train, acc: 0.998600, loss: 0.044261\n",
      "epoch: 63, val, acc: 0.998900, loss: 0.055338\n",
      "epoch: 64, train, acc: 0.998900, loss: 0.043900\n",
      "epoch: 64, val, acc: 0.998800, loss: 0.054872\n",
      "epoch: 65, train, acc: 0.998500, loss: 0.043523\n",
      "epoch: 65, val, acc: 0.999000, loss: 0.054412\n",
      "epoch: 66, train, acc: 0.998800, loss: 0.043175\n",
      "epoch: 66, val, acc: 0.998900, loss: 0.053971\n",
      "epoch: 67, train, acc: 0.998800, loss: 0.042841\n",
      "epoch: 67, val, acc: 0.998900, loss: 0.053536\n",
      "epoch: 68, train, acc: 0.998700, loss: 0.042498\n",
      "epoch: 68, val, acc: 0.998900, loss: 0.053115\n",
      "epoch: 69, train, acc: 0.998700, loss: 0.042166\n",
      "epoch: 69, val, acc: 0.998900, loss: 0.052702\n",
      "epoch: 70, train, acc: 0.998400, loss: 0.041848\n",
      "epoch: 70, val, acc: 0.998900, loss: 0.052297\n",
      "epoch: 71, train, acc: 0.998900, loss: 0.041543\n",
      "epoch: 71, val, acc: 0.998800, loss: 0.051908\n",
      "epoch: 72, train, acc: 0.998500, loss: 0.041234\n",
      "epoch: 72, val, acc: 0.999000, loss: 0.051518\n",
      "epoch: 73, train, acc: 0.999000, loss: 0.040935\n",
      "epoch: 73, val, acc: 0.998900, loss: 0.051145\n",
      "epoch: 74, train, acc: 0.998500, loss: 0.040638\n",
      "epoch: 74, val, acc: 0.999000, loss: 0.050775\n",
      "epoch: 75, train, acc: 0.999000, loss: 0.040353\n",
      "epoch: 75, val, acc: 0.998800, loss: 0.050419\n",
      "epoch: 76, train, acc: 0.998600, loss: 0.040079\n",
      "epoch: 76, val, acc: 0.998900, loss: 0.050066\n",
      "epoch: 77, train, acc: 0.998800, loss: 0.039802\n",
      "epoch: 77, val, acc: 0.998900, loss: 0.049723\n",
      "epoch: 78, train, acc: 0.998600, loss: 0.039535\n",
      "epoch: 78, val, acc: 0.998900, loss: 0.049383\n",
      "epoch: 79, train, acc: 0.999100, loss: 0.039275\n",
      "epoch: 79, val, acc: 0.998800, loss: 0.049059\n",
      "epoch: 80, train, acc: 0.998500, loss: 0.039016\n",
      "epoch: 80, val, acc: 0.998800, loss: 0.048734\n",
      "epoch: 81, train, acc: 0.998400, loss: 0.038774\n",
      "epoch: 81, val, acc: 0.998900, loss: 0.048412\n",
      "epoch: 82, train, acc: 0.998900, loss: 0.038516\n",
      "epoch: 82, val, acc: 0.998900, loss: 0.048103\n",
      "epoch: 83, train, acc: 0.998700, loss: 0.038273\n",
      "epoch: 83, val, acc: 0.998900, loss: 0.047796\n",
      "epoch: 84, train, acc: 0.998500, loss: 0.038042\n",
      "epoch: 84, val, acc: 0.999000, loss: 0.047495\n",
      "epoch: 85, train, acc: 0.998700, loss: 0.037810\n",
      "epoch: 85, val, acc: 0.999000, loss: 0.047202\n",
      "epoch: 86, train, acc: 0.998900, loss: 0.037591\n",
      "epoch: 86, val, acc: 0.999000, loss: 0.046915\n",
      "epoch: 87, train, acc: 0.998900, loss: 0.037356\n",
      "epoch: 87, val, acc: 0.998900, loss: 0.046636\n",
      "epoch: 88, train, acc: 0.998500, loss: 0.037136\n",
      "epoch: 88, val, acc: 0.998900, loss: 0.046356\n",
      "epoch: 89, train, acc: 0.998900, loss: 0.036925\n",
      "epoch: 89, val, acc: 0.998900, loss: 0.046085\n",
      "epoch: 90, train, acc: 0.998700, loss: 0.036709\n",
      "epoch: 90, val, acc: 0.998900, loss: 0.045817\n",
      "epoch: 91, train, acc: 0.998500, loss: 0.036501\n",
      "epoch: 91, val, acc: 0.999000, loss: 0.045553\n",
      "epoch: 92, train, acc: 0.998800, loss: 0.036296\n",
      "epoch: 92, val, acc: 0.999000, loss: 0.045295\n",
      "epoch: 93, train, acc: 0.998800, loss: 0.036094\n",
      "epoch: 93, val, acc: 0.999100, loss: 0.045041\n",
      "epoch: 94, train, acc: 0.998900, loss: 0.035896\n",
      "epoch: 94, val, acc: 0.999000, loss: 0.044792\n",
      "epoch: 95, train, acc: 0.998900, loss: 0.035706\n",
      "epoch: 95, val, acc: 0.999000, loss: 0.044548\n",
      "epoch: 96, train, acc: 0.998900, loss: 0.035508\n",
      "epoch: 96, val, acc: 0.998900, loss: 0.044309\n",
      "epoch: 97, train, acc: 0.998500, loss: 0.035323\n",
      "epoch: 97, val, acc: 0.998900, loss: 0.044071\n",
      "epoch: 98, train, acc: 0.998500, loss: 0.035137\n",
      "epoch: 98, val, acc: 0.999000, loss: 0.043836\n",
      "epoch: 99, train, acc: 0.998700, loss: 0.034953\n",
      "epoch: 99, val, acc: 0.999100, loss: 0.043606\n",
      "epoch: 100, train, acc: 0.998500, loss: 0.034790\n",
      "epoch: 100, val, acc: 0.999100, loss: 0.043380\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs+1):\n",
    "    model.train()\n",
    "    y_trues = []\n",
    "    y_preds = []\n",
    "    losses = []\n",
    "    for x, y in train_dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred: Tensor = model(x)\n",
    "        loss: Tensor = loss_fn(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        y_trues.append(y)\n",
    "        y_preds.append(y_pred.argmax(dim=1))\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    acc = (torch.cat(y_trues) == torch.cat(y_preds)).type(torch.float).mean().item()\n",
    "    loss_mean = torch.tensor(losses).mean().item()\n",
    "    print(f\"epoch: {epoch}, train, acc: {acc:.6f}, loss: {loss_mean:.6f}\")\n",
    "\n",
    "    model.eval()\n",
    "    y_trues = []\n",
    "    y_preds = []\n",
    "    losses = []\n",
    "    for x, y in val_dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        with torch.no_grad():\n",
    "            y_pred: Tensor = model(x)\n",
    "        loss: Tensor = loss_fn(y_pred, y)\n",
    "        y_trues.append(y)\n",
    "        y_preds.append(y_pred.argmax(dim=1))\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    acc = (torch.cat(y_trues) == torch.cat(y_preds)).type(torch.float).mean().item()\n",
    "    loss_mean = torch.tensor(losses).mean().item()\n",
    "    print(f\"epoch: {epoch}, val, acc: {acc:.6f}, loss: {loss_mean:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000e-04, 1.0975e-04, 1.2045e-04, 1.3219e-04, 1.4508e-04, 1.5923e-04,\n",
       "        1.7475e-04, 1.9179e-04, 2.1049e-04, 2.3101e-04, 2.5354e-04, 2.7826e-04,\n",
       "        3.0539e-04, 3.3516e-04, 3.6784e-04, 4.0370e-04, 4.4306e-04, 4.8626e-04,\n",
       "        5.3367e-04, 5.8570e-04, 6.4281e-04, 7.0548e-04, 7.7426e-04, 8.4975e-04,\n",
       "        9.3260e-04, 1.0235e-03, 1.1233e-03, 1.2328e-03, 1.3530e-03, 1.4850e-03,\n",
       "        1.6298e-03, 1.7886e-03, 1.9630e-03, 2.1544e-03, 2.3645e-03, 2.5950e-03,\n",
       "        2.8480e-03, 3.1257e-03, 3.4305e-03, 3.7649e-03, 4.1320e-03, 4.5349e-03,\n",
       "        4.9770e-03, 5.4623e-03, 5.9948e-03, 6.5793e-03, 7.2208e-03, 7.9248e-03,\n",
       "        8.6975e-03, 9.5455e-03, 1.0476e-02, 1.1498e-02, 1.2619e-02, 1.3849e-02,\n",
       "        1.5199e-02, 1.6681e-02, 1.8307e-02, 2.0092e-02, 2.2051e-02, 2.4201e-02,\n",
       "        2.6561e-02, 2.9151e-02, 3.1993e-02, 3.5112e-02, 3.8535e-02, 4.2292e-02,\n",
       "        4.6416e-02, 5.0941e-02, 5.5908e-02, 6.1359e-02, 6.7342e-02, 7.3907e-02,\n",
       "        8.1113e-02, 8.9022e-02, 9.7701e-02, 1.0723e-01, 1.1768e-01, 1.2915e-01,\n",
       "        1.4175e-01, 1.5557e-01, 1.7074e-01, 1.8738e-01, 2.0565e-01, 2.2570e-01,\n",
       "        2.4771e-01, 2.7186e-01, 2.9836e-01, 3.2745e-01, 3.5938e-01, 3.9442e-01,\n",
       "        4.3288e-01, 4.7508e-01, 5.2140e-01, 5.7224e-01, 6.2803e-01, 6.8926e-01,\n",
       "        7.5646e-01, 8.3022e-01, 9.1116e-01, 1.0000e+00])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x = torch.logspace(-4, 0, 100)\n",
    "test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200, 1])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x = torch.cat([test_x, -test_x]).reshape(-1, 1)\n",
    "test_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y = (test_x > 0).type(torch.long).flatten()\n",
    "test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.inference_mode():\n",
    "    y_pred = model(test_x.to(device)).cpu().argmax(dim=-1)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.824999988079071"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 准确率\n",
    "(test_y == y_pred).type(torch.float).mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获取错误的 x index\n",
    "error_index = (test_y != y_pred)\n",
    "error_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000e-04, 1.0975e-04, 1.2045e-04, 1.3219e-04, 1.4508e-04, 1.5923e-04,\n",
       "        1.7475e-04, 1.9179e-04, 2.1049e-04, 2.3101e-04, 2.5354e-04, 2.7826e-04,\n",
       "        3.0539e-04, 3.3516e-04, 3.6784e-04, 4.0370e-04, 4.4306e-04, 4.8626e-04,\n",
       "        5.3367e-04, 5.8570e-04, 6.4281e-04, 7.0548e-04, 7.7426e-04, 8.4975e-04,\n",
       "        9.3260e-04, 1.0235e-03, 1.1233e-03, 1.2328e-03, 1.3530e-03, 1.4850e-03,\n",
       "        1.6298e-03, 1.7886e-03, 1.9630e-03, 2.1544e-03, 2.3645e-03])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获取错误的 x\n",
    "error_x = test_x[error_index].flatten()\n",
    "error_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
