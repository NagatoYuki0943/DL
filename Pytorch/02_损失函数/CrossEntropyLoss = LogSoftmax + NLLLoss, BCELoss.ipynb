{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 2, 1, 4, 1, 0, 0, 4, 0, 3])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "y_true = torch.randint(0, 5, (10,))\n",
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8823, 0.9150, 0.3829, 0.9593, 0.3904],\n",
       "        [0.6009, 0.2566, 0.7936, 0.9408, 0.1332],\n",
       "        [0.9346, 0.5936, 0.8694, 0.5677, 0.7411],\n",
       "        [0.4294, 0.8854, 0.5739, 0.2666, 0.6274],\n",
       "        [0.2696, 0.4414, 0.2969, 0.8317, 0.1053],\n",
       "        [0.2695, 0.3588, 0.1994, 0.5472, 0.0062],\n",
       "        [0.9516, 0.0753, 0.8860, 0.5832, 0.3376],\n",
       "        [0.8090, 0.5779, 0.9040, 0.5547, 0.3423],\n",
       "        [0.6343, 0.3644, 0.7104, 0.9464, 0.7890],\n",
       "        [0.2814, 0.7886, 0.5895, 0.7539, 0.1952]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "y_pred = torch.rand(10, 5)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 手动实现 CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropyloss(\n",
    "    y_pred: torch.Tensor, y_true: torch.Tensor, reduction=\"mean\"\n",
    ") -> Tensor:\n",
    "    \"\"\"交叉熵损失\n",
    "        result = - log(softmax(y_pred)) * one_hot(y_true)\n",
    "\n",
    "    Args:\n",
    "        input (Tensor):  predict value\n",
    "        target (Tensor): target value\n",
    "        reduction (str, optional): mean' | 'sum' | 'none'. Defaults to 'mean'.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: 交叉熵损失结果\n",
    "    \"\"\"\n",
    "\n",
    "    y_true = nn.functional.one_hot(y_true)\n",
    "    y_pred = torch.softmax(y_pred, dim=-1)\n",
    "    y_pred = -torch.log(y_pred)\n",
    "    result = y_pred * y_true\n",
    "\n",
    "    if reduction == \"sum\":\n",
    "        return result.sum()\n",
    "    elif reduction == \"mean\":\n",
    "        return result.sum() / y_true.size(0)\n",
    "    elif reduction == \"none\":\n",
    "        max, _ = result.max(dim=-1)\n",
    "        return max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.6211)\n",
      "tensor(1.6211)\n",
      "tensor(1.6211)\n"
     ]
    }
   ],
   "source": [
    "print(nn.CrossEntropyLoss(reduction=\"mean\")(y_pred, y_true))\n",
    "print(F.cross_entropy(y_pred, y_true, reduction=\"mean\"))\n",
    "print(cross_entropyloss(y_pred, y_true, reduction=\"mean\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(16.2108)\n",
      "tensor(16.2108)\n",
      "tensor(16.2108)\n"
     ]
    }
   ],
   "source": [
    "print(nn.CrossEntropyLoss(reduction=\"sum\")(y_pred, y_true))\n",
    "print(F.cross_entropy(y_pred, y_true, reduction=\"sum\"))\n",
    "print(cross_entropyloss(y_pred, y_true, reduction=\"sum\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.9654, 1.4071, 1.7677, 1.5602, 1.5892, 1.6320, 1.2770, 1.9243, 1.6820,\n",
      "        1.4059])\n",
      "tensor([1.9654, 1.4071, 1.7677, 1.5602, 1.5892, 1.6320, 1.2770, 1.9243, 1.6820,\n",
      "        1.4059])\n",
      "tensor([1.9654, 1.4071, 1.7677, 1.5602, 1.5892, 1.6320, 1.2770, 1.9243, 1.6820,\n",
      "        1.4059])\n"
     ]
    }
   ],
   "source": [
    "print(nn.CrossEntropyLoss(reduction=\"none\")(y_pred, y_true))\n",
    "print(F.cross_entropy(y_pred, y_true, reduction=\"none\"))\n",
    "print(cross_entropyloss(y_pred, y_true, reduction=\"none\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CrossEntropyLoss = LogSoftmax + NLLLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.6211)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.nll_loss(torch.log_softmax(y_pred, dim=-1), y_true, reduction=\"mean\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogSoftmax = Log(Softmax(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.4660, -1.4332, -1.9654, -1.3889, -1.9578],\n",
       "        [-1.5999, -1.9442, -1.4071, -1.2600, -2.0676],\n",
       "        [-1.4267, -1.7677, -1.4919, -1.7936, -1.6202],\n",
       "        [-1.7582, -1.3022, -1.6137, -1.9210, -1.5602],\n",
       "        [-1.7609, -1.5892, -1.7336, -1.1989, -1.9252],\n",
       "        [-1.6320, -1.5427, -1.7022, -1.3543, -1.8954],\n",
       "        [-1.2770, -2.1533, -1.3426, -1.6454, -1.8909],\n",
       "        [-1.4577, -1.6887, -1.3627, -1.7120, -1.9243],\n",
       "        [-1.6820, -1.9519, -1.6059, -1.3699, -1.5273],\n",
       "        [-1.8784, -1.3712, -1.5703, -1.4059, -1.9645]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.log(torch.softmax(y_pred, dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.6211)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.nll_loss(torch.log(torch.softmax(y_pred, dim=-1)), y_true, reduction=\"mean\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## label_smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_smoothing = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.6227)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.CrossEntropyLoss(label_smoothing=label_smoothing)(y_pred, y_true)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 尝试复现label_smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1.],\n",
       "        [0., 1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true_ont_hot = F.one_hot(y_true)\n",
    "y_true_ont_hot = y_true_ont_hot.type(torch.float32)\n",
    "y_true_ont_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False,  True, False, False],\n",
       "        [False, False,  True, False, False],\n",
       "        [False,  True, False, False, False],\n",
       "        [False, False, False, False,  True],\n",
       "        [False,  True, False, False, False],\n",
       "        [ True, False, False, False, False],\n",
       "        [ True, False, False, False, False],\n",
       "        [False, False, False, False,  True],\n",
       "        [ True, False, False, False, False],\n",
       "        [False, False, False,  True, False]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true_ont_hot == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_ont_hot[y_true_ont_hot == 1] = 1 - label_smoothing\n",
    "y_true_ont_hot[y_true_ont_hot == 0] = label_smoothing / (5 - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0250, 0.0250, 0.9000, 0.0250, 0.0250],\n",
       "        [0.0250, 0.0250, 0.9000, 0.0250, 0.0250],\n",
       "        [0.0250, 0.9000, 0.0250, 0.0250, 0.0250],\n",
       "        [0.0250, 0.0250, 0.0250, 0.0250, 0.9000],\n",
       "        [0.0250, 0.9000, 0.0250, 0.0250, 0.0250],\n",
       "        [0.9000, 0.0250, 0.0250, 0.0250, 0.0250],\n",
       "        [0.9000, 0.0250, 0.0250, 0.0250, 0.0250],\n",
       "        [0.0250, 0.0250, 0.0250, 0.0250, 0.9000],\n",
       "        [0.9000, 0.0250, 0.0250, 0.0250, 0.0250],\n",
       "        [0.0250, 0.0250, 0.0250, 0.9000, 0.0250]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true_ont_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 5]), torch.Size([10, 5]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.shape, y_true_ont_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.6227)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.CrossEntropyLoss(label_smoothing=label_smoothing)(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.6231)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 结果不同\n",
    "nn.CrossEntropyLoss()(y_pred, y_true_ont_hot)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 多类别分类(一个目标有多个标签)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 1, 0, 0],\n",
       "        [0, 0, 1, 0, 0],\n",
       "        [0, 1, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 1],\n",
       "        [0, 1, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 1],\n",
       "        [1, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 1, 0]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true_ont_hot_ = F.one_hot(y_true)\n",
    "y_true_ont_hot_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 1, 0, 0],\n",
       "        [0, 1, 1, 0, 0],\n",
       "        [0, 1, 1, 0, 0],\n",
       "        [1, 0, 0, 0, 1],\n",
       "        [0, 1, 0, 1, 0],\n",
       "        [1, 0, 0, 1, 0],\n",
       "        [1, 1, 0, 0, 0],\n",
       "        [0, 0, 1, 0, 1],\n",
       "        [1, 1, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 0]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 伪造多类别\n",
    "y_true_ont_hot_[0, 1] = 1\n",
    "y_true_ont_hot_[1, 1:3] = 1\n",
    "y_true_ont_hot_[2, 2] = 1\n",
    "y_true_ont_hot_[3, 0] = 1\n",
    "y_true_ont_hot_[4, 3] = 1\n",
    "y_true_ont_hot_[5, 3] = 1\n",
    "y_true_ont_hot_[6, 0:2] = 1\n",
    "y_true_ont_hot_[7, 2] = 1\n",
    "y_true_ont_hot_[8, 1] = 1\n",
    "y_true_ont_hot_[9, 0:3] = 1\n",
    "y_true_ont_hot_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 手动实现 BCELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bce_loss(y_pred: torch.Tensor, y_true: torch.Tensor, reduction=\"mean\") -> Tensor:\n",
    "    \"\"\"二分类交叉熵损失\n",
    "        result = - log(y_pred) * y_true - log(1 - y_pred) * (1 - y_true)\n",
    "\n",
    "    Args:\n",
    "        input (Tensor):  predict value\n",
    "        target (Tensor): target value\n",
    "        reduction (str, optional): mean' | 'sum' | 'none'. Defaults to 'mean'.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: 二分类交叉熵损失结果\n",
    "    \"\"\"\n",
    "\n",
    "    result = -torch.log(y_pred) * y_true - torch.log(1 - y_pred) * (1 - y_true)\n",
    "\n",
    "    if reduction == \"sum\":\n",
    "        return result.sum()\n",
    "    elif reduction == \"mean\":\n",
    "        return result.sum() / y_true.numel()\n",
    "    elif reduction == \"none\":\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3567)\n",
      "tensor(0.3567)\n",
      "tensor(0.3567)\n",
      "tensor(0.5514)\n",
      "tensor(0.5514)\n",
      "tensor(0.5514)\n"
     ]
    }
   ],
   "source": [
    "#       y_pred, y_true\n",
    "#       0.7      1\n",
    "#       0.3      0\n",
    "# sum   1        1\n",
    "# 两者相同\n",
    "print(bce_loss(torch.tensor(0.7), torch.tensor(1)))\n",
    "print(bce_loss(torch.tensor(0.3), torch.tensor(0)))\n",
    "print(\n",
    "    -torch.log(torch.tensor(0.7)) * torch.tensor(1)\n",
    "    - torch.log(torch.tensor(0.3)) * torch.tensor(0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5514)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#       y_pred, y_true\n",
    "#       0.6      0.9\n",
    "#       0.4      0.1\n",
    "# sum   1        1\n",
    "# 两者相同\n",
    "print(bce_loss(torch.tensor(0.6), torch.tensor(0.9)))\n",
    "print(bce_loss(torch.tensor(0.4), torch.tensor(0.1)))\n",
    "print(\n",
    "    -torch.log(torch.tensor(0.6)) * torch.tensor(0.9)\n",
    "    - torch.log(torch.tensor(0.4)) * torch.tensor(0.1)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BCELoss = Binary Cross Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.6231)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 交叉熵也可以计算,不过多个类别之间是互斥的\n",
    "nn.CrossEntropyLoss()(y_pred, y_true_ont_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9070)\n",
      "tensor(0.9070)\n"
     ]
    }
   ],
   "source": [
    "# BCELoss计算时的 pred 和 target 的形状要相同\n",
    "print(nn.BCELoss(reduction=\"mean\")(torch.sigmoid(y_pred), y_true_ont_hot))\n",
    "print(bce_loss(torch.sigmoid(y_pred), y_true_ont_hot, reduction=\"mean\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(45.3489)\n",
      "tensor(45.3489)\n"
     ]
    }
   ],
   "source": [
    "# BCELoss计算时的 pred 和 target 的形状要相同\n",
    "print(nn.BCELoss(reduction=\"sum\")(torch.sigmoid(y_pred), y_true_ont_hot))\n",
    "print(bce_loss(torch.sigmoid(y_pred), y_true_ont_hot, reduction=\"sum\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.2065, 1.2290, 0.5582, 1.2597, 0.8975],\n",
      "        [1.0230, 0.8232, 0.4524, 1.2468, 0.7586],\n",
      "        [1.2425, 0.4991, 1.1978, 1.0026, 1.1123],\n",
      "        [0.9200, 1.2087, 1.0064, 0.8286, 0.4906],\n",
      "        [0.8303, 0.5408, 0.8452, 1.1723, 0.7446],\n",
      "        [0.5944, 0.8796, 0.7928, 0.9900, 0.6961],\n",
      "        [0.4217, 0.7296, 1.2091, 1.0121, 0.8677],\n",
      "        [1.1571, 1.0088, 1.2214, 0.9946, 0.5708],\n",
      "        [0.4889, 0.8828, 1.0924, 1.2507, 1.1438],\n",
      "        [0.8367, 1.1436, 1.0160, 0.4610, 0.7906]])\n",
      "tensor([[1.2065, 1.2290, 0.5582, 1.2597, 0.8975],\n",
      "        [1.0230, 0.8232, 0.4524, 1.2468, 0.7586],\n",
      "        [1.2425, 0.4991, 1.1978, 1.0026, 1.1123],\n",
      "        [0.9200, 1.2087, 1.0064, 0.8286, 0.4906],\n",
      "        [0.8303, 0.5408, 0.8452, 1.1723, 0.7446],\n",
      "        [0.5944, 0.8796, 0.7928, 0.9900, 0.6961],\n",
      "        [0.4217, 0.7296, 1.2091, 1.0121, 0.8677],\n",
      "        [1.1571, 1.0088, 1.2214, 0.9946, 0.5708],\n",
      "        [0.4889, 0.8828, 1.0924, 1.2507, 1.1438],\n",
      "        [0.8367, 1.1436, 1.0160, 0.4610, 0.7906]])\n"
     ]
    }
   ],
   "source": [
    "# BCELoss计算时的 pred 和 target 的形状要相同\n",
    "print(nn.BCELoss(reduction=\"none\")(torch.sigmoid(y_pred), y_true_ont_hot))\n",
    "print(bce_loss(torch.sigmoid(y_pred), y_true_ont_hot, reduction=\"none\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.5679)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 交叉熵也可以计算,不过多个类别之间是互斥的\n",
    "nn.CrossEntropyLoss()(y_pred, y_true_ont_hot_.type(torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7695)\n",
      "tensor(0.7695)\n"
     ]
    }
   ],
   "source": [
    "# BCELoss计算时的 pred 和 target 的形状要相同\n",
    "print(\n",
    "    nn.BCELoss(reduction=\"mean\")(\n",
    "        torch.sigmoid(y_pred), y_true_ont_hot_.type(torch.float32)\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    bce_loss(\n",
    "        torch.sigmoid(y_pred), y_true_ont_hot_.type(torch.float32), reduction=\"mean\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(38.4762)\n",
      "tensor(38.4762)\n"
     ]
    }
   ],
   "source": [
    "# BCELoss计算时的 pred 和 target 的形状要相同\n",
    "print(\n",
    "    nn.BCELoss(reduction=\"sum\")(\n",
    "        torch.sigmoid(y_pred), y_true_ont_hot_.type(torch.float32)\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    bce_loss(\n",
    "        torch.sigmoid(y_pred), y_true_ont_hot_.type(torch.float32), reduction=\"sum\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.2286, 0.3368, 0.5199, 1.2837, 0.9073],\n",
      "        [1.0381, 0.5731, 0.3731, 1.2703, 0.7620],\n",
      "        [1.2659, 0.4398, 0.3501, 1.0168, 1.1308],\n",
      "        [0.5013, 1.2308, 1.0207, 0.8353, 0.4278],\n",
      "        [0.8370, 0.4966, 0.8526, 0.3614, 0.7472],\n",
      "        [0.5675, 0.8886, 0.7978, 0.4565, 0.6962],\n",
      "        [0.3265, 0.6562, 1.2312, 1.0267, 0.8762],\n",
      "        [1.1773, 1.0233, 0.3400, 1.0084, 0.5366],\n",
      "        [0.4255, 0.5275, 1.1102, 1.2744, 1.1635],\n",
      "        [0.5623, 0.3746, 0.4412, 0.3856, 0.7955]])\n",
      "tensor([[1.2286, 0.3368, 0.5199, 1.2837, 0.9073],\n",
      "        [1.0381, 0.5731, 0.3731, 1.2703, 0.7620],\n",
      "        [1.2659, 0.4398, 0.3501, 1.0168, 1.1308],\n",
      "        [0.5013, 1.2308, 1.0207, 0.8353, 0.4278],\n",
      "        [0.8370, 0.4966, 0.8526, 0.3614, 0.7472],\n",
      "        [0.5675, 0.8886, 0.7978, 0.4565, 0.6962],\n",
      "        [0.3265, 0.6562, 1.2312, 1.0267, 0.8762],\n",
      "        [1.1773, 1.0233, 0.3400, 1.0084, 0.5366],\n",
      "        [0.4255, 0.5275, 1.1102, 1.2744, 1.1635],\n",
      "        [0.5623, 0.3746, 0.4412, 0.3856, 0.7955]])\n"
     ]
    }
   ],
   "source": [
    "# BCELoss计算时的 pred 和 target 的形状要相同\n",
    "print(\n",
    "    nn.BCELoss(reduction=\"none\")(\n",
    "        torch.sigmoid(y_pred), y_true_ont_hot_.type(torch.float32)\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    bce_loss(\n",
    "        torch.sigmoid(y_pred), y_true_ont_hot_.type(torch.float32), reduction=\"none\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BCEWithLogitsLoss = Sigmoid + BCELoss\n",
    "\n",
    "This loss combines a `Sigmoid` layer and the `BCELoss` in one single class. This version is more numerically stable than using a plain `Sigmoid` followed by a `BCELoss` as, by combining the operations into one layer we take advantage of the log-sum-exp trick for numerical stability.\n",
    "\n",
    "该损失将 `Sigmoid` 层和 `BCELoss` 组合在一个类中。 这个版本比使用简单的 `Sigmoid` 后跟 `BCELoss` 在数值上更稳定，因为通过将操作组合到一层，我们利用 log-sum-exp 技巧来实现数值稳定性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9070)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BCELoss计算时的 pred 和 target 的形状要相同\n",
    "nn.BCEWithLogitsLoss()(y_pred, y_true_ont_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7695)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BCELoss计算时的 pred 和 target 的形状要相同\n",
    "nn.BCEWithLogitsLoss()(y_pred, y_true_ont_hot_.type(torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
