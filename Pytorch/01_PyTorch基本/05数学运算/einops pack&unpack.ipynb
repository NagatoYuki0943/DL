{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# einops.pack and einops.unpack\n",
    "\n",
    "https://github.com/arogozhnikov/einops/blob/master/docs/4-pack-and-unpack.ipynb\n",
    "\n",
    "einops 0.6 introduces two more functions to the family: `pack` and `unpack`.\n",
    "\n",
    "Here is what they do:\n",
    "\n",
    "- `unpack` reverses `pack`\n",
    "- `pack` reverses `unpack`\n",
    "\n",
    "Enlightened with this exhaustive description, let's move to examples.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# we'll use numpy for demo purposes\n",
    "# operations work the same way with other frameworks\n",
    "import numpy as np\n",
    "from einops import pack, unpack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking data layers\n",
    "\n",
    "Assume we have RGB image along with a corresponding depth image that we want to stack:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100, 200, 3), (100, 200))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h, w = 100, 200\n",
    "# image_rgb is 3-dimensional (h, w, 3) and depth is 2-dimensional (h, w)\n",
    "image_rgb = np.random.random([h, w, 3])\n",
    "image_depth = np.random.random([h, w])\n",
    "image_rgb.shape, image_depth.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# but we can stack them\n",
    "image_rgbd, ps = pack([image_rgb, image_depth], \"h w *\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to read packing patterns\n",
    "\n",
    "pattern `h w *` means that\n",
    "- output is 3-dimensional\n",
    "- first two axes (`h` and `w`) are shared across all inputs and also shared with output\n",
    "- inputs, however do not have to be 3-dimensional. They can be 2-dim, 3-dim, 4-dim, etc. <br/>\n",
    "  Regardless of inputs dimensionality, they all will be packed into 3-dim output, and information about how they were packed is stored in `PS`\n",
    "\n",
    "---\n",
    "\n",
    "模式 `h w *` 意味着\n",
    "- 输出是3维的\n",
    "- 前两个轴（`h` 和 `w`）在所有输入之间共享，也与输出共享\n",
    "- 但输入不必是 3 维的。 它们可以是 2 维、3 维、4 维等。 <br/>\n",
    "  无论输入维度如何，它们都将被打包成 3 维输出，并且有关它们如何打包的信息存储在 `PS` 中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100, 200, 3), (100, 200), (100, 200, 4))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# as you see, pack properly appended depth as one more layer\n",
    "# and correctly aligned axes!\n",
    "# this won't work off the shelf with np.concatenate or torch.cat or alike\n",
    "image_rgb.shape, image_depth.shape, image_rgbd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3,), ()]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now let's see what PS keeps.\n",
    "# PS means Packed Shapes, not PlayStation or Post Script\n",
    "ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which reads: first tensor had shape `h, w, 3`, while second tensor had shape `h, w`.\n",
    "That's just enough to reverse packing:\n",
    "\n",
    "---\n",
    "\n",
    "其中内容为：第一个张量的形状为 `h, w, 3`，而第二个张量的形状为 `h, w`。\n",
    "这足以反转打包："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100, 200, 3), (100, 200))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove 1-axis in depth image during unpacking. Results are (h, w, 3) and (h, w)\n",
    "unpacked_rgb, unpacked_depth = unpack(image_rgbd, ps, \"h w *\")\n",
    "unpacked_rgb.shape, unpacked_depth.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can unpack tensor in different ways manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100, 200, 3), (100, 200, 1))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# simple unpack by splitting the axis. Results are (h, w, 3) and (h, w, 1)\n",
    "rgb, depth = unpack(image_rgbd, [[3], [1]], \"h w *\")\n",
    "rgb.shape, depth.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100, 200, 2), (100, 200, 2))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# different split, both outputs have shape (h, w, 2)\n",
    "rg, bd = unpack(image_rgbd, [[2], [2]], \"h w *\")\n",
    "rg.shape, bd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100, 200), (100, 200), (100, 200), (100, 200))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unpack to 4 tensors of shape (h, w). More like 'unstack over last axis'\n",
    "[r, g, b, d] = unpack(image_rgbd, [[], [], [], []], \"h w *\")\n",
    "r.shape, g.shape, b.shape, d.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 处理单个array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "h, w = 100, 200\n",
    "# image_rgb is 3-dimensional (h, w, 3) and depth is 2-dimensional (h, w)\n",
    "image_rgb = np.random.random([h, w, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 100, 200, 3)\n",
      "[()]\n",
      "(100, 200, 3)\n"
     ]
    }
   ],
   "source": [
    "# 处理单个array,放入一个数组中\n",
    "image_bhwc, ps = pack([image_rgb], \"* h w c\")\n",
    "print(image_bhwc.shape)\n",
    "print(ps)\n",
    "\n",
    "print(unpack(image_bhwc, ps, \"* h w c\")[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Short summary so far\n",
    "\n",
    "- `einops.pack` is a 'more generic concatenation' (that can stack too)\n",
    "- `einops.unpack` is a 'more generic split'\n",
    "\n",
    "And, of course, `einops` functions are more verbose, and *reversing* concatenation now is *dead simple*\n",
    "\n",
    "Compared to other `einops` functions, `pack` and `unpack` have a compact pattern without arrow, and the same pattern can be used in `pack` and `unpack`. These patterns are very simplistic: just a sequence of space-separated axes names.\n",
    "One axis is `*`, all other axes are valid identifiers.\n",
    "\n",
    "Now let's discuss some practical cases\n",
    "\n",
    "---\n",
    "\n",
    "- `einops.pack` 是一个“更通用的concatenation”（也可以stack）\n",
    "- `einops.unpack` 是一个“更通用的分割”\n",
    "\n",
    "当然，`einops` 函数更加冗长，并且 **反转** 连接现 **非常简单**\n",
    "\n",
    "与其他 `einops` 函数相比， `pack` 和 `unpack` 使用没有箭头的紧凑模式，并且可以在 `pack` 和 `unpack` 中使用相同的模式。 这些模式非常简单：只是一系列以空格分隔的轴名称。\n",
    "一个轴是 `*`，所有其他轴都是有效标识符。\n",
    "\n",
    "现在我们来讨论一些实际案例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto-batching\n",
    "\n",
    "ML models by default accept batches: batch of images, or batch of sentences, or batch of audios, etc.\n",
    "\n",
    "During debugging or inference, however, it is common to pass a single image instead (and thus output should be a single prediction) <br />\n",
    "In this example we'll write `universal_predict` that can handle both cases.\n",
    "\n",
    "---\n",
    "\n",
    "ML 模型默认接受批次：批次图像、批次句子、批次音频等。\n",
    "\n",
    "然而，在调试或推理过程中，通常会传递单个图像（因此输出应该是单个预测）<br />\n",
    "在这个例子中，我们将编写可以处理这两种情况的 `universal_predict`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from einops import reduce\n",
    "\n",
    "\n",
    "def image_classifier(images_bhwc):\n",
    "    # mock for image classifier\n",
    "    predictions = reduce(images_bhwc, \"b h w c -> b c\", \"mean\", h=h, w=w, c=3)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def universal_predict(x):\n",
    "    x_packed, ps = pack([x], \"* h w c\")  # make any shape to [b, c, h, w]\n",
    "    print(x_packed.shape, ps)\n",
    "    predictions_packed = image_classifier(x_packed)\n",
    "    [predictions] = unpack(predictions_packed, ps, \"* cls\")  # revert shape\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 100, 200, 3) [()]\n",
      "(3,)\n",
      "(5, 100, 200, 3) [(5,)]\n",
      "(5, 3)\n",
      "(35, 100, 200, 3) [(5, 7)]\n",
      "(5, 7, 3)\n"
     ]
    }
   ],
   "source": [
    "# works with a single image\n",
    "print(universal_predict(np.zeros([h, w, 3])).shape)\n",
    "# works with a batch of images\n",
    "batch = 5\n",
    "print(universal_predict(np.zeros([batch, h, w, 3])).shape)\n",
    "# or even a batch of videos\n",
    "n_frames = 7\n",
    "print(universal_predict(np.zeros([batch, n_frames, h, w, 3])).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**what we can learn from this example**:\n",
    "\n",
    "- `pack` and `unpack` play nicely together. That's not a coincidence :)\n",
    "- patterns in `pack` and `unpack` may differ, and that's quite common for applications\n",
    "- unlike other operations in `einops`, `(un)pack` does not provide arbitrary reordering of axes\n",
    "\n",
    "**我们可以从这个例子中学到什么**：\n",
    "\n",
    "- `pack` 和 `unpack` 配合得很好。 这不是巧合:)\n",
    "- `pack` 和 `unpack` 中的模式可能不同，这对于应用程序来说很常见\n",
    "- 与 `einops` 中的其他操作不同，`(un)pack` 不提供轴的任意重新排序"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class token in VIT\n",
    "\n",
    "Let's assume we have a simple transformer model that works with `BTC`-shaped tensors.\n",
    "\n",
    "---\n",
    "\n",
    "假设我们有一个简单的 transformer 模型，可以使用 `BTC` 形状的张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def transformer_mock(x_btc):\n",
    "    # imagine this is a transformer model, a very efficient one\n",
    "    assert len(x_btc.shape) == 3\n",
    "    return x_btc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement vision transformer (ViT) with a class token (i.e. static token, corresponding output is used to classify an image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# below it is assumed that you already\n",
    "# 1) split batch of images into patches 2) applied linear projection and 3) used positional embedding.\n",
    "\n",
    "# We'll skip that here. But hey, here is an einops-style way of doing all of that in a single shot!\n",
    "# from einops.layers.torch import EinMix\n",
    "# patcher_and_posembedder = EinMix('b (h h2) (w w2) c -> b h w c_out', weight_shape='h2 w2 c c_out',\n",
    "#                                  bias_shape='h w c_out', h2=..., w2=...)\n",
    "# patch_tokens_bhwc = patcher_and_posembedder(images_bhwc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# preparations\n",
    "batch, height, width, c = 6, 16, 16, 256\n",
    "patch_tokens = np.random.random([batch, height, width, c])\n",
    "class_tokens = np.zeros([batch, c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 257, 256) [(), (16, 16)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((6, 256), (6, 16, 16, 256))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def vit_einops(class_tokens, patch_tokens):\n",
    "    input_packed, ps = pack([class_tokens, patch_tokens], \"b * c\")\n",
    "    print(input_packed.shape, ps)\n",
    "    output_packed = transformer_mock(input_packed)\n",
    "    return unpack(output_packed, ps, \"b * c_out\")\n",
    "\n",
    "\n",
    "class_token_emb, patch_tokens_emb = vit_einops(class_tokens, patch_tokens)\n",
    "\n",
    "class_token_emb.shape, patch_tokens_emb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, let's make a small pause and understand conveniences of this pipeline, by contrasting it to more 'standard' code\n",
    "\n",
    "---\n",
    "\n",
    "此时，让我们稍作停顿，通过将其与更“标准”的代码进行对比来了解该管道的便利性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def vit_vanilla(class_tokens, patch_tokens):\n",
    "    b, h, w, c = patch_tokens.shape\n",
    "    class_tokens_b1c = class_tokens[:, None, :]  # [b, c] -> [b, 1, c]\n",
    "    patch_tokens_btc = np.reshape(\n",
    "        patch_tokens, [b, -1, c]\n",
    "    )  # [b, h, w, c] -> [b, h*w, c]\n",
    "    input_packed = np.concatenate(\n",
    "        [class_tokens_b1c, patch_tokens_btc], axis=1\n",
    "    )  # [b, 1, c] cat [b, h*w, c] = [b, 1+h*w, c]\n",
    "    output_packed = transformer_mock(input_packed)\n",
    "    class_token_emb = np.squeeze(\n",
    "        output_packed[:, :1, :], 1\n",
    "    )  # [b, 1+h*w, c] get [b, 1, c] -> [b, c]\n",
    "    patch_tokens_emb = np.reshape(\n",
    "        output_packed[:, 1:, :], [b, h, w, -1]\n",
    "    )  # [b, 1+h*w, c] get [b, h*w, c] -> [b, h, w, c]\n",
    "    return class_token_emb, patch_tokens_emb\n",
    "\n",
    "\n",
    "class_token_emb2, patch_tokens_emb2 = vit_vanilla(class_tokens, patch_tokens)\n",
    "assert np.allclose(class_token_emb, class_token_emb2)\n",
    "assert np.allclose(patch_tokens_emb, patch_tokens_emb2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notably, we have put all packing and unpacking, reshapes, adding and removing of dummy axes into a couple of lines.\n",
    "\n",
    "---\n",
    "\n",
    "值得注意的是，我们将所有打包和拆包、重塑、添加和删除虚拟轴放入几行中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packing different modalities together\n",
    "\n",
    "We can extend the previous example: it is quite common to mix elements of different types of inputs in transformers.\n",
    "\n",
    "The simples one is to mix tokens from all inputs:\n",
    "\n",
    "```python\n",
    "all_inputs = [text_tokens_btc, image_bhwc, task_token_bc, static_tokens_bnc]\n",
    "inputs_packed, ps = pack(all_inputs, 'b * c')\n",
    "```\n",
    "\n",
    "and you can `unpack` resulting tokens to the same structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packing data coming from different sources together\n",
    "\n",
    "Most notable example is of course GANs:\n",
    "\n",
    "```python\n",
    "input_ims, ps = pack([true_images, fake_images], '* h w c')\n",
    "true_pred, fake_pred = unpack(model(input_ims), ps, '* c')\n",
    "```\n",
    "`true_pred` and `fake_pred` are handled differently, that's why we separated them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting multiple outputs at the same time\n",
    "\n",
    "It is quite common to pack prediction of multiple target values into a single layer.\n",
    "\n",
    "This is more efficient, but code is less readable. For example, that's how detection code may look like:\n",
    "\n",
    "---\n",
    "\n",
    "将多个目标值的预测打包到单个层中是很常见的。\n",
    "\n",
    "这样效率更高，但代码可读性较差。 例如，检测代码可能如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def loss_detection(model_output_bhwc, mask_h: int, mask_w: int, n_classes: int):\n",
    "    output = model_output_bhwc\n",
    "\n",
    "    confidence = output[..., 0].sigmoid()\n",
    "    bbox_x_shift = output[..., 1].sigmoid()\n",
    "    bbox_y_shift = output[..., 2].sigmoid()\n",
    "    bbox_w = output[..., 3]\n",
    "    bbox_h = output[..., 4]\n",
    "    mask_logits = output[..., 5 : 5 + mask_h * mask_w]\n",
    "    mask_logits = mask_logits.reshape([*mask_logits.shape[:-1], mask_h, mask_w])\n",
    "    class_logits = output[..., 5 + mask_h * mask_w :]\n",
    "    assert class_logits.shape[-1] == n_classes, class_logits.shape[-1]\n",
    "\n",
    "    # downstream computations\n",
    "    return (\n",
    "        confidence,\n",
    "        bbox_x_shift,\n",
    "        bbox_y_shift,\n",
    "        bbox_h,\n",
    "        bbox_w,\n",
    "        mask_logits,\n",
    "        class_logits,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the same logic is implemented in einops, there is no need to memorize offsets. <br />\n",
    "Additionally, reshapes and shape checks are automatic:\n",
    "\n",
    "---\n",
    "\n",
    "当在einops中实现相同的逻辑时，不需要记住偏移量。 <br/>\n",
    "此外，重塑和形状检查是自动的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def loss_detection_einops(model_output, mask_h: int, mask_w: int, n_classes: int):\n",
    "    (\n",
    "        confidence,\n",
    "        bbox_x_shift,\n",
    "        bbox_y_shift,\n",
    "        bbox_w,\n",
    "        bbox_h,\n",
    "        mask_logits,\n",
    "        class_logits,\n",
    "    ) = unpack(model_output, [[]] * 5 + [[mask_h, mask_w], [n_classes]], \"b h w *\")\n",
    "\n",
    "    confidence = confidence.sigmoid()\n",
    "    bbox_x_shift = bbox_x_shift.sigmoid()\n",
    "    bbox_y_shift = bbox_y_shift.sigmoid()\n",
    "\n",
    "    # downstream computations\n",
    "    return (\n",
    "        confidence,\n",
    "        bbox_x_shift,\n",
    "        bbox_y_shift,\n",
    "        bbox_h,\n",
    "        bbox_w,\n",
    "        mask_logits,\n",
    "        class_logits,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# check that results are identical\n",
    "import torch\n",
    "\n",
    "dims = dict(mask_h=6, mask_w=8, n_classes=19)\n",
    "model_output = torch.randn(\n",
    "    [3, 5, 7, 5 + dims[\"mask_h\"] * dims[\"mask_w\"] + dims[\"n_classes\"]]\n",
    ")\n",
    "for a, b in zip(\n",
    "    loss_detection(model_output, **dims), loss_detection_einops(model_output, **dims)\n",
    "):\n",
    "    assert torch.allclose(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or maybe **reinforcement learning** is closer to your mind?\n",
    "\n",
    "If so, predicting multiple outputs is valuable there too:\n",
    "\n",
    "```python\n",
    "action_logits, reward_expectation, q_values, expected_entropy_after_action = \\\n",
    "    unpack(predictions_btc, [[n_actions], [], [n_actions], [n_actions]], 'b step *')\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "## That's all for today!\n",
    "\n",
    "happy packing and unpacking!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
