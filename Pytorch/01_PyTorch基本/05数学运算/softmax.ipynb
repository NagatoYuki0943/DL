{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 重新审视Softmax的实现\n",
    ":label:`subsec_softmax-implementation-revisited`\n",
    "\n",
    "在前面 :numref:`sec_softmax_scratch`的例子中，\n",
    "我们计算了模型的输出，然后将此输出送入交叉熵损失。\n",
    "从数学上讲，这是一件完全合理的事情。\n",
    "然而，从计算角度来看，指数可能会造成数值稳定性问题。\n",
    "\n",
    "回想一下，softmax函数$\\hat y_j = \\frac{\\exp(o_j)}{\\sum_k \\exp(o_k)}$，\n",
    "其中$\\hat y_j$是预测的概率分布。\n",
    "$o_j$是未规范化的预测$\\mathbf{o}$的第$j$个元素。\n",
    "如果$o_k$中的一些数值非常大，\n",
    "那么$\\exp(o_k)$可能大于数据类型容许的最大数字，即*上溢*（overflow）。\n",
    "这将使分母或分子变为`inf`（无穷大），\n",
    "最后得到的是0、`inf`或`nan`（不是数字）的$\\hat y_j$。\n",
    "在这些情况下，我们无法得到一个明确定义的交叉熵值。\n",
    "\n",
    "解决这个问题的一个技巧是：\n",
    "在继续softmax计算之前，先从所有$o_k$中减去$\\max(o_k)$。\n",
    "这里可以看到每个$o_k$按常数进行的移动不会改变softmax的返回值：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat y_j & =  \\frac{\\exp(o_j - \\max(o_k))\\exp(\\max(o_k))}{\\sum_k \\exp(o_k - \\max(o_k))\\exp(\\max(o_k))} \\\\\n",
    "& = \\frac{\\exp(o_j - \\max(o_k))}{\\sum_k \\exp(o_k - \\max(o_k))}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "在减法和规范化步骤之后，可能有些$o_j - \\max(o_k)$具有较大的负值。\n",
    "由于精度受限，$\\exp(o_j - \\max(o_k))$将有接近零的值，即*下溢*（underflow）。\n",
    "这些值可能会四舍五入为零，使$\\hat y_j$为零，\n",
    "并且使得$\\log(\\hat y_j)$的值为`-inf`。\n",
    "反向传播几步后，我们可能会发现自己面对一屏幕可怕的`nan`结果。\n",
    "\n",
    "尽管我们要计算指数函数，但我们最终在计算交叉熵损失时会取它们的对数。\n",
    "通过将softmax和交叉熵结合在一起，可以避免反向传播过程中可能会困扰我们的数值稳定性问题。\n",
    "如下面的等式所示，我们避免计算$\\exp(o_j - \\max(o_k))$，\n",
    "而可以直接使用$o_j - \\max(o_k)$，因为$\\log(\\exp(\\cdot))$被抵消了。\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\log{(\\hat y_j)} & = \\log\\left( \\frac{\\exp(o_j - \\max(o_k))}{\\sum_k \\exp(o_k - \\max(o_k))}\\right) \\\\\n",
    "& = \\log{(\\exp(o_j - \\max(o_k)))}-\\log{\\left( \\sum_k \\exp(o_k - \\max(o_k)) \\right)} \\\\\n",
    "& = o_j - \\max(o_k) -\\log{\\left( \\sum_k \\exp(o_k - \\max(o_k)) \\right)}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "我们也希望保留传统的softmax函数，以备我们需要评估通过模型输出的概率。\n",
    "但是，我们没有将softmax概率传递到损失函数中，\n",
    "而是[**在交叉熵损失函数中传递未规范化的预测，并同时计算softmax及其对数**]，\n",
    "这是一种类似[\"LogSumExp技巧\"](https://en.wikipedia.org/wiki/LogSumExp)的聪明方式。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_softmax(x: np.ndarray):\n",
    "    x = x - x.max() # 减去最大值保持稳定\n",
    "    x = np.exp(x)\n",
    "    sum = x.sum(axis=-1, keepdims=True) # 保存维度不变\n",
    "    return x / sum # 这里应用了广播机制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pytorch_softmax(x: torch.Tensor):\n",
    "    x = x - x.max() # 减去最大值保持稳定\n",
    "    x = x.exp()\n",
    "    sum = x.sum(dim=-1, keepdim=True)   # 保存维度不变\n",
    "    return x / sum # 这里应用了广播机制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2,  3,  4,  5],\n",
       "       [ 6,  7,  8,  9, 10]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.arange(1, 11).reshape(2, 5)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01165623, 0.03168492, 0.08612854, 0.23412166, 0.63640865],\n",
       "       [0.01165623, 0.03168492, 0.08612854, 0.23412166, 0.63640865]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = numpy_softmax(x)\n",
    "x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.sum(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0117, 0.0317, 0.0861, 0.2341, 0.6364],\n",
       "        [0.0117, 0.0317, 0.0861, 0.2341, 0.6364]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2 = pytorch_softmax(torch.from_numpy(x))\n",
    "x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2.sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
