{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import spatial\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector1 = np.array([1., 1])\n",
    "vector2 = np.array([-1., 1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cosine_similarity(vector1, vector2) = (vector1 / l2norm(vector1)) @ (vector2 / l2norm(vector2)).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(vector1, vector2) / (np.linalg.norm(vector1) * np.linalg.norm(vector2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector1 @ vector2.T / (np.linalg.norm(vector1) * np.linalg.norm(vector2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector1 @ vector2.T / (np.sqrt(np.sum(vector1 ** 2)) * np.sqrt(np.sum(vector2 ** 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 先除以各自的norm再计算矩阵相乘\n",
    "(vector1 / np.linalg.norm(vector1)) @ (vector2 / np.linalg.norm(vector2)).T"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - spatial.distance.cosine(vector1, vector2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1., 1.]]), array([[-1.,  1.]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector1.reshape(1, -1), vector2.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(vector1.reshape(1, -1), vector2.reshape(1, -1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 1.], dtype=torch.float64),\n",
       " tensor([-1.,  1.], dtype=torch.float64))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor1 = torch.from_numpy(vector1)\n",
    "tensor2 = torch.from_numpy(vector2)\n",
    "tensor1, tensor2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., dtype=torch.float64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cosine_similarity(tensor1, tensor2, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., dtype=torch.float64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.dot(tensor1, tensor2) / (tensor1.norm() * tensor2.norm())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_60820\\4149563073.py:1: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at ..\\aten\\src\\ATen\\native\\TensorShape.cpp:3641.)\n",
      "  tensor1 @ tensor2.T / (tensor1.norm() * tensor2.norm())\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0., dtype=torch.float64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor1 @ tensor2.T / (tensor1.norm() * tensor2.norm())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., dtype=torch.float64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 先除以各自的norm再计算矩阵相乘\n",
    "(tensor1 / tensor1.norm()) @ (tensor2 / tensor2.norm()).T"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix\n",
    "\n",
    "## cosine_similarity(matrix1, matrix2) = (matrix1 / l2norm(matrix1)) @ (matrix2 / l2norm(matrix2)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix1 = np.array([[1., 1],\n",
    "                    [1., 0]])\n",
    "matrix2 = np.array([[-1., 1],\n",
    "                    [2., 1],\n",
    "                    [-1., -1]])\n",
    "# cos value     matrix2[0]  matrix2[1]  matrix2[2]\n",
    "# matrix1[0]    0           0.9487      -1\n",
    "# matrix1[1]    -0.7071     0.8944      -0.7071"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.41421356]\n",
      " [1.        ]]\n",
      "[[1.41421356 2.23606798 1.41421356]]\n"
     ]
    }
   ],
   "source": [
    "print(np.linalg.norm(matrix1, axis=-1, keepdims=True))\n",
    "print(np.linalg.norm(matrix2, axis=-1, keepdims=True).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.        , 3.16227766, 2.        ],\n",
       "       [1.41421356, 2.23606798, 1.41421356]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(matrix1, axis=-1, keepdims=True) @ np.linalg.norm(matrix2, axis=-1, keepdims=True).T # 这里用 * 或者 @ 结果相同, * 使用了广播机制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.9486833 , -1.        ],\n",
       "       [-0.70710678,  0.89442719, -0.70710678]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix1 @ matrix2.T / (np.linalg.norm(matrix1, axis=-1, keepdims=True) @ np.linalg.norm(matrix2, axis=-1, keepdims=True).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.23711432e-17,  9.48683298e-01, -1.00000000e+00],\n",
       "       [-7.07106781e-01,  8.94427191e-01, -7.07106781e-01]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 先除以各自的norm再计算矩阵相乘\n",
    "(matrix1 / np.linalg.norm(matrix1, axis=-1, keepdims=True)) @ (matrix2 / np.linalg.norm(matrix2, axis=-1, keepdims=True)).T"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.23711432e-17,  9.48683298e-01, -1.00000000e+00],\n",
       "       [-7.07106781e-01,  8.94427191e-01, -7.07106781e-01]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(matrix1, matrix2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 1.],\n",
       "         [1., 0.]], dtype=torch.float64),\n",
       " tensor([[-1.,  1.],\n",
       "         [ 2.,  1.],\n",
       "         [-1., -1.]], dtype=torch.float64))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor1 = torch.from_numpy(matrix1)\n",
    "tensor2 = torch.from_numpy(matrix2)\n",
    "tensor1, tensor2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 1.],\n",
      "         [1., 0.]]], dtype=torch.float64)\n",
      "tensor([[[-1.,  1.]],\n",
      "\n",
      "        [[ 2.,  1.]],\n",
      "\n",
      "        [[-1., -1.]]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(tensor1.unsqueeze(0))\n",
    "print(tensor2.unsqueeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.9487, -1.0000],\n",
       "        [-0.7071,  0.8944, -0.7071]], dtype=torch.float64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 要添加维度并转置结果\n",
    "F.cosine_similarity(tensor1.unsqueeze(0), tensor2.unsqueeze(1), dim=-1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.0000, 3.1623, 2.0000],\n",
       "        [1.4142, 2.2361, 1.4142]], dtype=torch.float64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor1.norm(p=2, dim=-1, keepdim=True) @ tensor2.norm(p=2, dim=-1, keepdim=True).T # 这里用 * 或者 @ 结果相同, * 使用了广播机制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.9487, -1.0000],\n",
       "        [-0.7071,  0.8944, -0.7071]], dtype=torch.float64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对于多维多来说每个值都要除以对应vector的norm的乘积\n",
    "tensor1 @ tensor2.T / (tensor1.norm(p=2, dim=-1, keepdim=True) @ tensor2.norm(p=2, dim=-1, keepdim=True).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.9487, -1.0000],\n",
       "        [-0.7071,  0.8944, -0.7071]], dtype=torch.float64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 可以先除以各自的norm再计算矩阵相乘\n",
    "(tensor1 / (tensor1.norm(p=2, dim=-1, keepdim=True))) @ (tensor2 / tensor2.norm(p=2, dim=-1, keepdim=True)).T"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector&Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector1 = np.array([1., 1])\n",
    "matrix1 = np.array([[-1.,  1],\n",
    "                    [2.,   1],\n",
    "                    [-1., -1]])\n",
    "# cos value     matrix2[0]  matrix2[1]  matrix2[2]\n",
    "# vector1       0           0.9487      -1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.        , 3.16227766, 2.        ])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.linalg.norm(vector1, axis=-1, keepdims=True) @ np.linalg.norm(matrix1, axis=-1, keepdims=True).T) # 这里用 * 或者 @ 结果相同, * 使用了广播机制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.       ,  0.9486833, -1.       ])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector1 @ matrix1.T / (np.linalg.norm(vector1, axis=-1, keepdims=True) @ np.linalg.norm(matrix1, axis=-1, keepdims=True).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.23711432e-17,  9.48683298e-01, -1.00000000e+00])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 可以先除以各自的norm再计算矩阵相乘\n",
    "(vector1 / np.linalg.norm(vector1, axis=-1, keepdims=True)) @ (matrix1 / np.linalg.norm(matrix1, axis=-1, keepdims=True)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.23711432e-17,  9.48683298e-01, -1.00000000e+00])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 颠倒前后顺序\n",
    "(matrix1 / np.linalg.norm(matrix1, axis=-1, keepdims=True)) @ (vector1 / np.linalg.norm(vector1, axis=-1, keepdims=True)).T"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.23711432e-17,  9.48683298e-01, -1.00000000e+00]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(vector1.reshape(1, -1), matrix1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.23711432e-17],\n",
       "       [ 9.48683298e-01],\n",
       "       [-1.00000000e+00]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 颠倒前后顺序\n",
    "cosine_similarity(matrix1, vector1.reshape(1, -1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor1 = torch.from_numpy(vector1)\n",
    "tensor2 = torch.from_numpy(matrix1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0000,  0.9487, -1.0000], dtype=torch.float64)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 要添加维度并转置结果\n",
    "F.cosine_similarity(tensor1.unsqueeze(0), tensor2, dim=-1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.0000, 3.1623, 2.0000], dtype=torch.float64)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor1.norm(p=2, dim=-1, keepdim=True) @ tensor2.norm(p=2, dim=-1, keepdim=True).T # 这里用 * 或者 @ 结果相同, * 使用了广播机制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0000,  0.9487, -1.0000], dtype=torch.float64)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor1 @ tensor2.T / (tensor1.norm(dim=-1, p=2, keepdim=True) @ tensor2.norm(p=2, dim=-1, keepdim=True).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0000,  0.9487, -1.0000], dtype=torch.float64)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 可以先除以各自的norm再计算矩阵相乘\n",
    "tensor1 / (tensor1.norm(dim=-1, p=2, keepdim=True)) @ (tensor2 / tensor2.norm(p=2, dim=-1, keepdim=True)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0000,  0.9487, -1.0000], dtype=torch.float64)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 颠倒前后顺序\n",
    "(tensor2 / tensor2.norm(p=2, dim=-1, keepdim=True)) @ (tensor1 / tensor1.norm(p=2, dim=-1, keepdim=True)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
