{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import models\n",
    "from torchvision.ops.misc import MLP\n",
    "from typing import List, Optional, Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(2, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (0): Linear(in_features=10, out_features=20, bias=False)\n",
       "  (1): ReLU(inplace=True)\n",
       "  (2): Dropout(p=0.5, inplace=True)\n",
       "  (3): Linear(in_features=20, out_features=30, bias=False)\n",
       "  (4): ReLU(inplace=True)\n",
       "  (5): Dropout(p=0.5, inplace=True)\n",
       "  (6): Linear(in_features=30, out_features=40, bias=False)\n",
       "  (7): Dropout(p=0.5, inplace=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 一般norm_layer和dropout不同时使用\n",
    "mlp = MLP(\n",
    "    in_channels=10,\n",
    "    hidden_channels=[20, 30, 40],\n",
    "    norm_layer=None,\n",
    "    activation_layer=nn.ReLU,\n",
    "    inplace=True,\n",
    "    bias=False,\n",
    "    dropout=0.5,\n",
    ")\n",
    "mlp.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 40])\n"
     ]
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    y = mlp(x)\n",
    "print(y.size())  # [2, 40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torchvision.ops.misc.MLP\n",
    "class MLP1(torch.nn.Sequential):\n",
    "    \"\"\"This block implements the multi-layer perceptron (MLP) module.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Number of channels of the input\n",
    "        hidden_channels (List[int]): List of the hidden channel dimensions\n",
    "        norm_layer (Callable[..., torch.nn.Module], optional): Norm layer that will be stacked on top of the convolution layer. If ``None`` this layer wont be used. Default: ``None``\n",
    "        activation_layer (Callable[..., torch.nn.Module], optional): Activation function which will be stacked on top of the normalization layer (if not None), otherwise on top of the conv layer. If ``None`` this layer wont be used. Default: ``torch.nn.ReLU``\n",
    "        inplace (bool): Parameter for the activation layer, which can optionally do the operation in-place. Default ``True``\n",
    "        bias (bool): Whether to use bias in the linear layer. Default ``True``\n",
    "        dropout (float): The probability for the dropout layer. Default: 0.0\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        hidden_channels: List[int],\n",
    "        norm_layer: Optional[Callable[..., torch.nn.Module]] = None,\n",
    "        activation_layer: Optional[Callable[..., torch.nn.Module]] = torch.nn.ReLU,\n",
    "        inplace: Optional[bool] = True,\n",
    "        bias: bool = True,\n",
    "        dropout: float = 0.0,\n",
    "    ):\n",
    "        # The addition of `norm_layer` is inspired from the implementation of TorchMultimodal:\n",
    "        # https://github.com/facebookresearch/multimodal/blob/5dec8a/torchmultimodal/modules/layers/mlp.py\n",
    "        params = {} if inplace is None else {\"inplace\": inplace}\n",
    "\n",
    "        layers = []\n",
    "        in_dim = in_channels\n",
    "        for hidden_dim in hidden_channels[:-1]:  # 不要最后一层\n",
    "            layers.append(torch.nn.Linear(in_dim, hidden_dim, bias=bias))\n",
    "            if norm_layer is not None:\n",
    "                layers.append(norm_layer(hidden_dim))\n",
    "            layers.append(activation_layer(**params))\n",
    "            layers.append(torch.nn.Dropout(dropout, **params))\n",
    "            in_dim = hidden_dim\n",
    "\n",
    "        # layers.append(torch.nn.Linear(in_dim, hidden_channels[-1], bias=bias))\n",
    "        layers.append(\n",
    "            torch.nn.Linear(in_dim, hidden_channels[-1])\n",
    "        )  # 最后一层的bias设置为始终为True\n",
    "        layers.append(torch.nn.Dropout(dropout, **params))\n",
    "\n",
    "        super().__init__(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP1(\n",
       "  (0): Linear(in_features=10, out_features=20, bias=False)\n",
       "  (1): ReLU(inplace=True)\n",
       "  (2): Dropout(p=0.5, inplace=True)\n",
       "  (3): Linear(in_features=20, out_features=30, bias=False)\n",
       "  (4): ReLU(inplace=True)\n",
       "  (5): Dropout(p=0.5, inplace=True)\n",
       "  (6): Linear(in_features=30, out_features=40, bias=True)\n",
       "  (7): Dropout(p=0.5, inplace=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp1 = MLP1(\n",
    "    in_channels=10,\n",
    "    hidden_channels=[20, 30, 40],\n",
    "    norm_layer=None,\n",
    "    activation_layer=nn.ReLU,\n",
    "    inplace=True,\n",
    "    bias=False,\n",
    "    dropout=0.5,\n",
    ")\n",
    "mlp1.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 40])\n"
     ]
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    y = mlp1(x)\n",
    "print(y.size())  # [2, 40]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
